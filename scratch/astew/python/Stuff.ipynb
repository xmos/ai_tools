{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(action='default')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iinfo_s32 = np.iinfo(np.int32)\n",
    "iinfo_s16 = np.iinfo(np.int16)\n",
    "iinfo_s8 = np.iinfo(np.int8)\n",
    "\n",
    "bitdepth_map = { # because using \"np.int8\" directly as the key doesn't work\n",
    "    np.zeros(0,dtype=np.int8).dtype:   8,\n",
    "    np.zeros(0,dtype=np.int16).dtype: 16,\n",
    "    np.zeros(0,dtype=np.int32).dtype: 32,\n",
    "}\n",
    "\n",
    "unit_test_dir = \"../../../xcore/operator_book/test/nn_operators/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_tensor_s8(*dims):\n",
    "    return np.random.randint(-128, 128, size=tuple(dims), dtype=np.int8)\n",
    "def rand_tensor_s32(*dims):\n",
    "    return np.random.randint(iinfo_s32.min, np.int64(iinfo_s32.max)+1, size=tuple(dims), dtype=np.int32)\n",
    "\n",
    "def vlsat_s16(x, shift):\n",
    "    psh = shift > 0\n",
    "    nsh = shift < 0\n",
    "    x[...,psh] += 1 << (shift[psh]-1)\n",
    "    x[...,psh] = x[...,psh] >> shift[psh]\n",
    "    x[...,nsh] = x[...,nsh] << -shift[nsh]\n",
    "    return np.clip(x, a_min = iinfo_s16.min+1, a_max = iinfo_s16.max).astype(np.int16)\n",
    "    \n",
    "\n",
    "def vlmul_s16(x, scales):\n",
    "    assert(x.shape[-1] == scales.shape[-1])\n",
    "    while scales.ndim < x.ndim:\n",
    "        scales = np.expand_dims(scales, axis=0)\n",
    "    y = x.astype(np.int32) * scales\n",
    "    assert(y.dtype == np.int32)\n",
    "    #vlmul for s16 has an implicit built-in vlsat on \n",
    "    # the 32-bit result, where all shfits are 14\n",
    "    shifts = 14 *np.ones(y.shape, dtype=np.int16)\n",
    "    return vlsat_s16(y, shifts)\n",
    "\n",
    "def vdepth8(x):\n",
    "    shr = bitdepth_map[x.dtype] - 8\n",
    "    x = x.astype(np.int64)\n",
    "    assert(shr > 0)\n",
    "    x += 1<<(shr-1)\n",
    "    x = x >> shr\n",
    "    x = np.clip(x, a_min = iinfo_s8.min+1, a_max = iinfo_s8.max).astype(np.int8)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def conv2d_s8(K, X, zero_point=0):\n",
    "    C_out, K_h, K_w, C_in = K.shape\n",
    "    height, width, C_inx = X.shape\n",
    "    assert(K_h % 2 == 1)\n",
    "    assert(K_w % 2 == 1)\n",
    "    assert(C_in == C_inx)\n",
    "    P_h, P_w = (K_h//2),(K_w//2)\n",
    "    \n",
    "    X_padded = (np.ones(shape=(height+2*P_h, width+2*P_w, C_in), dtype=np.int8) * zero_point).astype(np.int8)\n",
    "    X_padded[P_h:height+P_h,P_w:width+P_w,:] = X\n",
    "    Y = np.zeros(shape=(height,width, C_out), dtype=np.int32)\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            patch = X_padded[i:i+K_h,j:j+K_w,:].astype(np.int32)\n",
    "            for k in range(C_out):\n",
    "                kernel = K[k,:,:,:]\n",
    "                Y[i,j,k] = np.sum(kernel * patch)\n",
    "    \n",
    "    return Y\n",
    "\n",
    "\n",
    "def inflate_BTensor(B, height, width):\n",
    "    K_h, K_w, C_out = B.shape\n",
    "    P = np.array((K_h//2, K_w//2))\n",
    "    D = (height,width)\n",
    "    Q = P - D + 1\n",
    "    \n",
    "    res = np.zeros(shape=(height, width, C_out), dtype=B.dtype)\n",
    "    \n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            \n",
    "            C = np.array((row, col))\n",
    "            \n",
    "            pad_sta = ( (P-C) > 0 ) * (P-C)\n",
    "            pad_sto = ( (C+Q) > 0 ) * (C+Q)\n",
    "            \n",
    "            br,bc = pad_sto - pad_sta + P\n",
    "            \n",
    "            res[row,col] = B[br,bc]\n",
    "            \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just testing inflate_BTensor()\n",
    "if False:\n",
    "    B = np.array([['ü°î','ü°ë','ü°ï'],['ü°ê','O','ü°í'],['ü°ó','ü°ì','ü°ñ']])[:,:,np.newaxis]\n",
    "\n",
    "    print(\"B:\\n\",B[:,:,0], \"\\n\\n===\")\n",
    "    res = inflate_BTensor(B, 3, 3)\n",
    "    print(res[:,:,0])\n",
    "    res = inflate_BTensor(B, 6, 10)\n",
    "    print(res[:,:,0])\n",
    "    res = inflate_BTensor(B, 4, 7)\n",
    "    print(res[:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maxpool2d_deep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maxpool2d_deep(tensor_in):\n",
    "    assert(tensor_in.ndim == 3)\n",
    "    return tf.nn.max_pool2d(td.expand_dims(tensor_in, axis=0),\n",
    "                            ksize=2, strides=2, padding='VALID'\n",
    "                           )[0,:,:,:].numpy()\n",
    "\n",
    "\n",
    "def test_case_maxpool2d_deep(width, height, chans, writefile=None):\n",
    "    #produce a maxpool2d_deep() test case\n",
    "    A = rand_tensor_s8(width, height, chans)\n",
    "\n",
    "    A_out = maxpool2d_deep(A)\n",
    "    if writefile:\n",
    "        with open(writefile, \"wb+\") as file:\n",
    "            A.tofile(file)\n",
    "            A_out.tofile(file)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fc_deepin_shallowout_lin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fc_deepin_shallowout_lin(W, B, X, shifts, scales):\n",
    "    y = np.matmul(W.astype(np.int32), X.astype(np.int32)) + B\n",
    "    assert(y.shape == shifts.shape)\n",
    "    y = vlsat_s16(y, shifts)\n",
    "    y = vlmul_s16(y, scales)\n",
    "    return y\n",
    "\n",
    "\n",
    "def test_case_fc_deepin_shallowout_lin(C_in, C_out, writefile=None):\n",
    "    #produce a fc_deepin_shallowout_lin() test case\n",
    "    W = rand_tensor_s8(C_out, C_in)\n",
    "    X = rand_tensor_s8(C_in)\n",
    "    B = rand_tensor_s32(C_out)\n",
    "    \n",
    "    shifts = np.random.randint(0, 16, size=C_out, dtype=np.int16)\n",
    "    scales = np.random.randint(0x4000, 0x8000, size=C_out, dtype=np.int16)\n",
    "    \n",
    "    #shouldn't let bias dominate the pre-activation value\n",
    "    if(1):\n",
    "        while(True):\n",
    "            tmp1 = np.matmul(W.astype(np.int32), X.astype(np.int32))\n",
    "            \n",
    "            too_biased = np.abs(tmp1) < np.abs(B)\n",
    "            \n",
    "            if np.sum(too_biased) <= (C_out / 10):\n",
    "                break\n",
    "                \n",
    "            B[too_biased] = (B[too_biased] * 0.5).astype(np.int32)\n",
    "        \n",
    "    \n",
    "    #going to iterate to make sure our shifts are reasonable\n",
    "    if(1):\n",
    "        #only allow <10% of values to be 0 \n",
    "        while(True):\n",
    "            tmp = np.matmul(W.astype(np.int32), X.astype(np.int32)) + B\n",
    "            tmp = vlsat_s16(tmp, shifts)\n",
    "\n",
    "            zeros = (tmp == 0)\n",
    "            if np.sum(zeros) <= (C_out / 10):\n",
    "                break\n",
    "                \n",
    "            shifts[zeros] = shifts[zeros] - 2\n",
    "\n",
    "        #only allow <10% of values to saturate\n",
    "        while(True):\n",
    "            tmp = np.matmul(W.astype(np.int32), X.astype(np.int32)) + B\n",
    "            tmp = vlsat_s16(tmp, shifts)\n",
    "            \n",
    "            sats = np.logical_or(tmp == iinfo_s16.max,tmp == (iinfo_s16.min+1))\n",
    "            if np.sum(sats) <= (C_out / 10):\n",
    "                break\n",
    "                \n",
    "            shifts[sats] = shifts[sats] + 2\n",
    "    \n",
    "    #going to iterate to make sure our scales are reasonable\n",
    "    if(1):\n",
    "        #only allow <10% of values to saturate\n",
    "        while(True):\n",
    "            tmp = np.matmul(W.astype(np.int32), X.astype(np.int32)) + B\n",
    "            tmp = vlsat_s16(tmp, shifts)\n",
    "            tmp = vlmul_s16(tmp, scales)\n",
    "\n",
    "            sats = np.logical_or(tmp == iinfo_s16.max,tmp == (iinfo_s16.min+1))\n",
    "            if np.sum(sats) <= (C_out / 10):\n",
    "                break\n",
    "                \n",
    "            scales[sats] = (scales[sats] * 0.9).astype(np.int16)\n",
    "        \n",
    "    \n",
    "    Y = fc_deepin_shallowout_lin(W, B, X, shifts, scales)\n",
    "    \n",
    "    if writefile:\n",
    "        with open(writefile, \"wb+\") as file:\n",
    "#             print(W.dtype, X.dtype, B.dtype, shifts.dtype, scales.dtype, Y.dtype)\n",
    "            assert(W.dtype == np.int8 and X.dtype == np.int8 and B.dtype == np.int32 \n",
    "                    and shifts.dtype == np.int16 and scales.dtype == np.int16\n",
    "                    and Y.dtype == np.int16)\n",
    "            W.tofile(file)\n",
    "            X.tofile(file)\n",
    "            B.tofile(file)\n",
    "            shifts.tofile(file)\n",
    "            scales.tofile(file)\n",
    "            (Y).tofile(file)\n",
    "    \n",
    "    return W,X,B,shifts,scales,Y\n",
    "\n",
    "\n",
    "#Generate test vectors for fc_deepin_shallowout_lin()\n",
    "if False:\n",
    "    thing = [\n",
    "        (2, 32, 4, 100),\n",
    "        (3, 96, 15, 100),\n",
    "    ]\n",
    "    for (caseNum, C_in, C_out, vecs) in thing:\n",
    "                \n",
    "        Ws      = np.zeros(shape=(0, C_out, C_in), dtype=np.int8)\n",
    "        Xs      = np.zeros(shape=(0, C_in), dtype=np.int8)\n",
    "        Bs      = np.zeros(shape=(0, C_out), dtype=np.int32)\n",
    "        shiftss = np.zeros(shape=(0, C_out), dtype=np.int16)\n",
    "        scaless = np.zeros(shape=(0, C_out), dtype=np.int16)\n",
    "        Ys      = np.zeros(shape=(0, C_out), dtype=np.int16)\n",
    "        \n",
    "        for i in range(vecs):\n",
    "            W,X,B,shifts,scales,Y = test_case_fc_deepin_shallowout_lin(C_in, C_out, \n",
    "                    writefile=os.path.join(unit_test_dir, \n",
    "                        \"test_data/fc_deepin_shallowout_lin_case{0}.{1}.dat\".format(caseNum, i)))\n",
    "            \n",
    "            Ws      = np.append(Ws,      W[np.newaxis,...],      axis=0)\n",
    "            Xs      = np.append(Xs,      X[np.newaxis,...],      axis=0)\n",
    "            Bs      = np.append(Bs,      B[np.newaxis,...],      axis=0)\n",
    "            shiftss = np.append(shiftss, shifts[np.newaxis,...], axis=0)\n",
    "            scaless = np.append(scaless, scales[np.newaxis,...], axis=0)\n",
    "            Ys      = np.append(Ys,      Y[np.newaxis,...],      axis=0)\n",
    "\n",
    "\n",
    "        #These will be used to ensure we're not passing the test because files weren't\n",
    "        # correctly loaded\n",
    "        ychk_str = (\"#undef Y_CHECK\\n#define Y_CHECK  \" \n",
    "                    +  \",\".join([str(x) for x in Ys[:,0]]) + \"\\n\")\n",
    "\n",
    "        with open(os.path.join(unit_test_dir, \n",
    "                               \"test_data/fc_deepin_shallowout_lin_case{0}.h\".format(caseNum)), \"w+\"\n",
    "                 ) as file:\n",
    "                  file.write(ychk_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv2d_deepin_deepout_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def conv2d_deepin_deepout_relu(K, B_tensor, X, shifts, scales, debug=False):\n",
    "    y = conv2d_s8(K, X)\n",
    "    y = y + inflate_BTensor(B_tensor, X.shape[0], X.shape[1])\n",
    "    \n",
    "    assert(y.dtype == np.int32)\n",
    "    y = vlsat_s16(y, shifts)\n",
    "    assert(y.dtype == np.int16)\n",
    "    y = vlmul_s16(y, scales)\n",
    "    assert(y.dtype == np.int16)\n",
    "    y = vdepth8(y)\n",
    "    assert(y.dtype == np.int8)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def test_case_conv2d_deepin_deepout_relu(height, width, K_h, K_w, C_in, C_out, writefile=None):\n",
    "    #produce a conv2d_deepin_deepout_relu() test case\n",
    "    \n",
    "    def formBTensor(B, K, zero_point=0):\n",
    "        B_tensor = B.reshape((1,1,C_out))\n",
    "        B_tensor = np.repeat(B_tensor, K_w, axis=1)\n",
    "        B_tensor = np.repeat(B_tensor, K_h, axis=0)\n",
    "        assert(B_tensor.shape == (K_h, K_w, C_out))\n",
    "        K_h_half, K_w_half = K_h//2, K_w//2\n",
    "        \n",
    "        img = np.zeros(shape=(K_h, K_w, C_in), dtype=np.int8)\n",
    "        \n",
    "        img = conv2d_s8(K, img, zero_point)\n",
    "        assert(img.dtype==np.int32)\n",
    "        \n",
    "        for kh in range(-K_h_half, K_h_half+1):\n",
    "            for kw in range(-K_w_half, K_w_half+1):\n",
    "                B_tensor[kh+K_h_half,kw+K_w_half,:] = B + img[kh+K_h_half,kw+K_w_half,:]\n",
    "        \n",
    "        return B_tensor\n",
    "\n",
    "    def transformK(K):\n",
    "        C_out,K_h,K_w,C_in = K.shape\n",
    "        assert(C_out % 16 == 0)\n",
    "        assert(C_in % 32 == 0)\n",
    "        assert(K_h % 2 == 1)\n",
    "        assert(K_w % 2 == 1)\n",
    "\n",
    "        Q = K.reshape((K.shape[0]//16, 16, K.shape[1], K.shape[2], K.shape[3]//32, 32))\n",
    "        Q = np.transpose(np.flip(Q, axis=1), axes=(0, 2, 3, 4, 1, 5)).flatten()\n",
    "        \n",
    "        return Q\n",
    "    \n",
    "    def transformB(B):\n",
    "        assert(B.dtype == np.int32)\n",
    "        assert(B.shape[:2] == (K_h, K_w))\n",
    "        \n",
    "        C_out = B.shape[2]\n",
    "                \n",
    "        B_out = np.zeros(shape=(K_h, K_w, 2, C_out), dtype=np.uint16)\n",
    "        \n",
    "        B_out[:,:,1,:] = B >> 16\n",
    "        B_out[:,:,0,:] = B\n",
    "        return B_out\n",
    "\n",
    "    K = rand_tensor_s8(C_out, K_h, K_w, C_in)\n",
    "    X = rand_tensor_s8(height, width, C_in)\n",
    "    zero_point = np.random.randint(-64, 64)\n",
    "    \n",
    "    tmp = conv2d_s8(K, X)\n",
    "    assert(tmp.dtype == np.int32)\n",
    "    B = -(np.mean(tmp, axis=(0,1))).astype(np.int32)\n",
    "    \n",
    "    B_tensor = formBTensor(B, K, zero_point)\n",
    "    \n",
    "    tmp = tmp + inflate_BTensor(B_tensor, height, width)\n",
    "    assert(tmp.dtype == np.int32)\n",
    "    \n",
    "    acc32s = np.array(tmp)\n",
    "    \n",
    "    tmp_min = np.min(tmp, axis=(0,1))\n",
    "    tmp_max = np.max(tmp, axis=(0,1))\n",
    "    tmp_max = np.max((tmp_max, np.abs(tmp_min)), axis=0)\n",
    "    shifts  = np.ceil(np.log2(tmp_max)).astype(np.int16) - 15\n",
    "    \n",
    "    scales = np.ones(C_out, dtype=np.int16) * 0x4000\n",
    "    \n",
    "    Y = conv2d_deepin_deepout_relu(K, B_tensor, X, shifts, scales)\n",
    "    \n",
    "    if writefile:\n",
    "        with open(writefile, \"wb+\") as file:\n",
    "            assert(K.dtype==np.int8);       assert(X.dtype==np.int8);  \n",
    "            assert(shifts.dtype==np.int16); assert(scales.dtype==np.int16); assert(Y.dtype==np.int8)\n",
    "                \n",
    "#             tK = transformK(K)\n",
    "#             for i in range(16):\n",
    "#                 assert(np.all(K[i,:,:,:] == tK[15-i,:,:,:]))\n",
    "            \n",
    "            transformK(K).tofile(file)\n",
    "            transformB(B_tensor).tofile(file)\n",
    "            X.tofile(file)\n",
    "            shifts.tofile(file)\n",
    "            scales.tofile(file)\n",
    "            Y.tofile(file)\n",
    "            acc32s.tofile(file)\n",
    "            \n",
    "    return K,B_tensor,X,shifts,scales,Y,acc32s\n",
    "\n",
    "#Generate test vectors for conv2d_deepin_deepout_relu()\n",
    "FNAME = \"conv2d_deepin_deepout_relu\"\n",
    "if True:\n",
    "    thing = [ #caseNum, C_in, C_out, K_h, K_w, height, width, numVectors\n",
    "        (2, 32, 16, 1, 1, 4, 4, 10),\n",
    "        (3, 32, 16, 3, 3, 4, 4, 10),\n",
    "        (4, 64, 32, 3, 3, 8, 8, 4),\n",
    "    ]\n",
    "    for (caseNum, C_in, C_out, K_h, K_w, height, width, vecs) in thing:\n",
    "        \n",
    "        Ks      = np.zeros(shape=(0, C_out, K_h, K_w, C_in), dtype=np.int8)\n",
    "        Xs      = np.zeros(shape=(0, height, width, C_in), dtype=np.int8)\n",
    "        Bs      = np.zeros(shape=(0, K_h, K_w, C_out), dtype=np.int32)\n",
    "        shiftss = np.zeros(shape=(0, C_out), dtype=np.int16)\n",
    "        scaless = np.zeros(shape=(0, C_out), dtype=np.int16)\n",
    "        Ys      = np.zeros(shape=(0, height, width, C_out), dtype=np.int8)\n",
    "        acc32s  = np.zeros(shape=(0, height, width, C_out), dtype=np.int32)\n",
    "        \n",
    "        for i in range(vecs):\n",
    "            K,B,X,shifts,scales,Y,acc32 = test_case_conv2d_deepin_deepout_relu(\n",
    "                    height, width, K_h, K_w, C_in, C_out,\n",
    "                    writefile=os.path.join(unit_test_dir, \n",
    "                        \"test_data/{0}_case{1}.{2}.dat\".format(FNAME, caseNum, i)))\n",
    "            Ks      = np.append(Ks,      K[np.newaxis,...],      axis=0)\n",
    "            Xs      = np.append(Xs,      X[np.newaxis,...],      axis=0)\n",
    "            Bs      = np.append(Bs,      B[np.newaxis,...],      axis=0)\n",
    "            shiftss = np.append(shiftss, shifts[np.newaxis,...], axis=0)\n",
    "            scaless = np.append(scaless, scales[np.newaxis,...], axis=0)\n",
    "            Ys      = np.append(Ys,      Y[np.newaxis,...],      axis=0)\n",
    "            acc32s  = np.append(acc32s,  acc32[np.newaxis,...],  axis=0)\n",
    "\n",
    "\n",
    "        #These will be used to ensure we're not passing the test because files weren't\n",
    "        # correctly loaded\n",
    "        y_vec_count_str = \"#undef TEST_VECTOR_COUNT\\n#define TEST_VECTOR_COUNT    ({0})\\n\".format(vecs)\n",
    "        ychk_str = (\"#undef Y_CHECK\\n#define Y_CHECK  \" \n",
    "                    +  \",\".join([str(x) for x in Ys[:,0,0,0]]) + \"\\n\")\n",
    "\n",
    "        with open(os.path.join(unit_test_dir, \n",
    "                               \"test_data/{0}_case{1}.h\".format(FNAME, caseNum)), \"w+\"\n",
    "                 ) as file:\n",
    "                  file.write(y_vec_count_str)\n",
    "                  file.write(ychk_str)\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv2d_shallowin_deepout_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def conv2d_shallowin_deepout_relu(K, B_tensor, X, shifts, scales):\n",
    "    C_out, K_h, K_w, C_in\n",
    "    assert(C_out % 16 == 0)\n",
    "    assert(C_in == 4)\n",
    "    assert(K_h % 2 == 1)\n",
    "    assert((K_w < 8) and (K_w % 2 == 1))\n",
    "    y = conv2d_s8(K, X)\n",
    "    y += inflate_BTensor(B_tensor, X.shape[0], X.shape[1])\n",
    "    \n",
    "    y = vlsat_s16(y, shifts)\n",
    "    assert(y.dtype == np.int16)\n",
    "    y = vlmul_s16(y, scales)\n",
    "    y = vdepth8(y)\n",
    "    assert(y.dtype == np.int8)\n",
    "    \n",
    "    return y\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def test_case_conv2d_shallowin_deepout_relu(height, width, K_h, K_w, C_in, C_out, writefile=None):\n",
    "    #produce a conv2d_deepin_deepout_relu() test case\n",
    "    assert(C_in == 4)\n",
    "    \n",
    "    def formBTensor(B, K, zero_point=0):\n",
    "        B_tensor = B.reshape((1,1,C_out))\n",
    "        B_tensor = np.repeat(B_tensor, K_w, axis=1)\n",
    "        B_tensor = np.repeat(B_tensor, K_h, axis=0)\n",
    "        assert(B_tensor.shape == (K_h, K_w, C_out))\n",
    "        K_h_half, K_w_half = K_h//2, K_w//2\n",
    "        \n",
    "        img = np.zeros(shape=(K_h, K_w, C_in), dtype=np.int8)\n",
    "        \n",
    "        img = conv2d_s8(K, img, zero_point)\n",
    "        assert(img.dtype==np.int32)\n",
    "        \n",
    "        for kh in range(-K_h_half, K_h_half+1):\n",
    "            for kw in range(-K_w_half, K_w_half+1):\n",
    "                B_tensor[kh+K_h_half,kw+K_w_half,:] = B + img[kh+K_h_half,kw+K_w_half,:]\n",
    "        \n",
    "        return B_tensor\n",
    "    \n",
    "    def transformK(K):\n",
    "        \"\"\"K layout needs to be:\n",
    "            - C_out group (i.e. C_out // 16)\n",
    "            - Row\n",
    "            - C_out index (i.e. C_out % 16)\n",
    "            - Col\n",
    "            - C_in\n",
    "        \"\"\"\n",
    "        cou, hei, wid, cin = K.shape\n",
    "        assert(cou % 16 == 0); \n",
    "        assert(wid <= 8);\n",
    "        assert(cin <= 4)\n",
    "        \n",
    "        cout_groups = cou // 16\n",
    "        \n",
    "        # Pad width to 8 and cin to 4\n",
    "        K = np.concatenate((K, np.zeros(shape=(cou, hei, 8-wid, cin), dtype=np.int8)),axis=2)\n",
    "        K = np.concatenate((K, np.zeros(shape=(cou, hei, 8, 4-cin), dtype=np.int8)), axis=3)\n",
    "        \n",
    "        K = K.reshape((cout_groups, 16, hei, 8, 4))\n",
    "        K = np.flip(K, axis=1).transpose(0,2,1,3,4)\n",
    "        \n",
    "        return K\n",
    "    \n",
    "    def transformB(B, K_h, K_w):\n",
    "        assert(B.dtype == np.int32)\n",
    "        assert(B.shape[:2] == (K_h, K_w))\n",
    "        C_out = B.shape[2]\n",
    "        B_out = np.zeros(shape=(K_h, K_w, 2, C_out), dtype=np.uint16)\n",
    "        B_out[:,:,1,:] = B >> 16\n",
    "        B_out[:,:,0,:] = B\n",
    "        return B_out\n",
    "        \n",
    "    \n",
    "    K = rand_tensor_s8(C_out, K_h, K_w, C_in)\n",
    "    X = rand_tensor_s8(height, width, C_in)\n",
    "    zero_point = np.random.randint(-64, 64)\n",
    "    \n",
    "    tmp = conv2d_s8(K, X)\n",
    "    B = -(np.mean(tmp, axis=(0,1))).astype(np.int32)\n",
    "    \n",
    "    B_tensor = formBTensor(B, K, zero_point)\n",
    "    \n",
    "    tmp = tmp + inflate_BTensor(B_tensor, height, width)\n",
    "    assert(tmp.dtype == np.int32)\n",
    "    \n",
    "    acc32s = np.array(tmp)\n",
    "    \n",
    "    tmp_min = np.min(tmp, axis=(0,1))\n",
    "    tmp_max = np.max(tmp, axis=(0,1))\n",
    "    tmp_max = np.max((tmp_max, np.abs(tmp_min)), axis=0)\n",
    "    shifts  = np.ceil(np.log2(tmp_max)).astype(np.int16) - 15\n",
    "    shifts  = np.clip(shifts, a_min=0, a_max=None)\n",
    "    \n",
    "    scales = np.ones(C_out, dtype=np.int16) * 0x4000\n",
    "    \n",
    "    Y = conv2d_shallowin_deepout_relu(K, B_tensor, X, shifts, scales)\n",
    "    \n",
    "    \n",
    "    if writefile:\n",
    "        with open(writefile, \"wb+\") as file:\n",
    "            assert(K.dtype==np.int8);       assert(X.dtype==np.int8);  \n",
    "            assert(shifts.dtype==np.int16); assert(scales.dtype==np.int16); assert(Y.dtype==np.int8)\n",
    "            transformK(K).tofile(file)\n",
    "            transformB(B_tensor, K_h, K_w).tofile(file)\n",
    "            X.tofile(file)\n",
    "            shifts.tofile(file)\n",
    "            scales.tofile(file)\n",
    "            Y.tofile(file)\n",
    "            acc32s.tofile(file)\n",
    "            \n",
    "    return K,B_tensor,X,shifts,scales,Y,acc32s\n",
    "\n",
    "\n",
    "#Generate test vectors for conv2d_shallowin_deepout_relu()\n",
    "FNAME = \"conv2d_shallowin_deepout_relu\"\n",
    "if True:\n",
    "    thing = [ #caseNum, C_in, C_out, K_h, K_w, height, width, numVectors\n",
    "        (2, 4, 16, 1, 1, 2, 2, 10),\n",
    "        (3, 4, 16, 3, 3, 4, 4, 10),\n",
    "        (4, 4, 32, 3, 3, 8, 8, 4),\n",
    "    ]\n",
    "    for (caseNum, C_in, C_out, K_h, K_w, height, width, vecs) in thing:\n",
    "        \n",
    "        Ks      = np.zeros(shape=(0, C_out, K_h, K_w, C_in), dtype=np.int8)\n",
    "        Xs      = np.zeros(shape=(0, height, width, C_in), dtype=np.int8)\n",
    "        Bs      = np.zeros(shape=(0, K_h, K_w, C_out), dtype=np.int32)\n",
    "        shiftss = np.zeros(shape=(0, C_out), dtype=np.int16)\n",
    "        scaless = np.zeros(shape=(0, C_out), dtype=np.int16)\n",
    "        Ys      = np.zeros(shape=(0, height, width, C_out), dtype=np.int8)\n",
    "        acc32s  = np.zeros(shape=(0, height, width, C_out), dtype=np.int32)\n",
    "        \n",
    "        for i in range(vecs):\n",
    "            K,B,X,shifts,scales,Y,acc32 = test_case_conv2d_shallowin_deepout_relu(\n",
    "                    height, width, K_h, K_w, C_in, C_out,\n",
    "                    writefile=os.path.join(unit_test_dir, \n",
    "                        \"test_data/{0}_case{1}.{2}.dat\".format(FNAME, caseNum, i)))\n",
    "            Ks      = np.append(Ks,      K[np.newaxis,...],      axis=0)\n",
    "            Xs      = np.append(Xs,      X[np.newaxis,...],      axis=0)\n",
    "            Bs      = np.append(Bs,      B[np.newaxis,...],      axis=0)\n",
    "            shiftss = np.append(shiftss, shifts[np.newaxis,...], axis=0)\n",
    "            scaless = np.append(scaless, scales[np.newaxis,...], axis=0)\n",
    "            Ys      = np.append(Ys,      Y[np.newaxis,...],      axis=0)\n",
    "            acc32s  = np.append(acc32s,  acc32[np.newaxis,...],  axis=0)\n",
    "\n",
    "\n",
    "        #These will be used to ensure we're not passing the test because files weren't\n",
    "        # correctly loaded\n",
    "        y_vec_count_str = \"#undef TEST_VECTOR_COUNT\\n#define TEST_VECTOR_COUNT    ({0})\\n\".format(vecs)\n",
    "        ychk_str = (\"#undef Y_CHECK\\n#define Y_CHECK  \" \n",
    "                    +  \",\".join([str(x) for x in Ys[:,0,0,0]]) + \"\\n\")\n",
    "\n",
    "        with open(os.path.join(unit_test_dir, \n",
    "                               \"test_data/{0}_case{1}.h\".format(FNAME, caseNum)), \"w+\"\n",
    "                 ) as file:\n",
    "                  file.write(y_vec_count_str)\n",
    "                  file.write(ychk_str)\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
