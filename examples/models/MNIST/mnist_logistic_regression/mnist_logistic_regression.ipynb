{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(\"Eager execution enabled: \", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and rescale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (\n",
    "    test_images,\n",
    "    test_labels,\n",
    ") = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "scale = tf.constant(255, dtype=tf.dtypes.float32)\n",
    "x_train, x_test = train_images / scale, test_images / scale\n",
    "y_train, y_test = tf.expand_dims(train_labels, 1), tf.expand_dims(test_labels, 1)\n",
    "\n",
    "# mean = tf.math.reduce_mean(x_train)\n",
    "# std = tf.math.reduce_std(x_train)\n",
    "# x_train, x_test = (x_train-mean)/std, (x_test-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define, compile, and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# single dense layer, i.e. multiple logistic regression\n",
    "def build_model():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            keras.layers.Dense(\n",
    "                10, activation=\"softmax\", kernel_regularizer=keras.regularizers.l1(1e-5)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "training_params = {\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "}\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "model = build_model()\n",
    "model.compile(**training_params)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TFLite and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = pathlib.Path(\"./models/\")\n",
    "models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model_float_lite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_float_file = models_dir / \"model_float.tflite\"\n",
    "size_float = model_float_file.write_bytes(model_float_lite)\n",
    "print(\"Float model size: {:.0f} KB\".format(size_float / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # this doesn't seem to do anything\n",
    "\n",
    "# representative dataset to estimate activation distributions\n",
    "x_train_ds = tf.data.Dataset.from_tensor_slices((x_train)).batch(1)\n",
    "\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value in x_train_ds.take(x_train.shape[0]):\n",
    "        yield [input_value]\n",
    "\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "model_quant_lite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant_file = models_dir / \"model_quant.tflite\"\n",
    "size_quant = model_quant_file.write_bytes(model_quant_lite)\n",
    "print(\"Quantized model size: {:.0f} KB\".format(size_quant / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build interpreters and run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_float = tf.lite.Interpreter(model_content=model_float_lite)\n",
    "interpreter_float.allocate_tensors()\n",
    "interpreter_quant = tf.lite.Interpreter(model_content=model_quant_lite)\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_float = np.NaN * np.zeros((y_test.shape[0], 10))\n",
    "probabilities_quant = np.NaN * np.zeros((y_test.shape[0], 10))\n",
    "probabilities = model(x_test).numpy()\n",
    "\n",
    "for j, img in enumerate(x_test):\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    interpreter_float.set_tensor(interpreter_float.get_input_details()[0][\"index\"], img)\n",
    "    interpreter_float.invoke()\n",
    "    probabilities_float[j] = interpreter_float.get_tensor(\n",
    "        interpreter_float.get_output_details()[0][\"index\"]\n",
    "    )\n",
    "\n",
    "    interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], img)\n",
    "    interpreter_quant.invoke()\n",
    "    probabilities_quant[j] = interpreter_quant.get_tensor(\n",
    "        interpreter_quant.get_output_details()[0][\"index\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_abs_err_float = norm(probabilities_float - probabilities, axis=1)\n",
    "prob_abs_err_quant = norm(probabilities_quant - probabilities, axis=1)\n",
    "denom = norm(probabilities, axis=1)\n",
    "prob_rel_err_float = prob_abs_err_float / denom\n",
    "prob_rel_err_quant = prob_abs_err_quant / denom\n",
    "print(\"Mean relative error of output activations compared to original model output:\")\n",
    "print(\"# Float TFLite model:     {:.5e}\".format(np.mean(prob_rel_err_float)))\n",
    "print(\"# Quantized TFLite model: {:.5e}\".format(np.mean(prob_rel_err_quant)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_float = np.argmax(probabilities_float, axis=1)\n",
    "predictions_quant = np.argmax(probabilities_quant, axis=1)\n",
    "predictions = np.argmax(probabilities, axis=1)\n",
    "\n",
    "acc = tf.metrics.Accuracy()\n",
    "print(\"Accuracy of models:\")\n",
    "print(\"# Original keras model:   {:.2%}\".format(acc(test_labels, predictions).numpy()))\n",
    "print(\n",
    "    \"# Float TFLite model:     {:.2%}\".format(\n",
    "        acc(test_labels, predictions_float).numpy()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"# Quantized TFLite model: {:.2%}\".format(\n",
    "        acc(test_labels, predictions_quant).numpy()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreter surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run interpreters on a single sample\n",
    "img = tf.expand_dims(x_test[10], 0)\n",
    "interpreter_float.set_tensor(interpreter_float.get_input_details()[0][\"index\"], img)\n",
    "interpreter_float.invoke()\n",
    "interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], img)\n",
    "interpreter_quant.invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float interpreter components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_float.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized interpreter components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_quant.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve input image and its quantization, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_float = interpreter_float.get_tensor(1)[0].copy()\n",
    "img_quant_float = interpreter_quant.get_tensor(5)[0].copy()\n",
    "img_quant_int8 = interpreter_quant.get_tensor(1)[0].copy()\n",
    "img_quantization = interpreter_quant.get_tensor_details()[1][\"quantization\"]\n",
    "\n",
    "img_quant_int8_float = (\n",
    "    np.float32(img_quant_int8) - img_quantization[1]\n",
    ") * img_quantization[0]\n",
    "img_quant_float_int8 = np.int8(\n",
    "    img_quant_float / img_quantization[0] + img_quantization[1]\n",
    ")\n",
    "img_quant_diff = np.abs(\n",
    "    (np.float32(img_quant_int8) - img_quantization[1]) * img_quantization[0]\n",
    "    - img_quant_float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dict = {\n",
    "    \"float input\": img_float,\n",
    "    \"quant float input\": img_quant_float,\n",
    "    \"quant int8 input\": img_quant_int8,\n",
    "    \"quant inputs' diff\": img_quant_diff,\n",
    "    \"float from quant int8\": img_quant_int8_float,\n",
    "    \"int8 from quant float\": img_quant_float_int8,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for j, (title, im) in enumerate(im_dict.items()):\n",
    "    plt.subplot(1, len(im_dict), j + 1)\n",
    "    kwargs = {\"vmin\": 0, \"vmax\": 1} if title == \"quant inputs' diff\" else dict()\n",
    "    plt.imshow(im, cmap=\"gray\", **kwargs)\n",
    "    plt.title(title)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate that the bug corrupts the internal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: file a bug report\n",
    "\n",
    "interpreter_quant.set_tensor(\n",
    "    interpreter_quant.get_input_details()[0][\"index\"],\n",
    "    tf.expand_dims(img_quant_int8_float, 0),\n",
    ")\n",
    "interpreter_quant.invoke()\n",
    "print(\"Output with corrupted image:\")\n",
    "print(\n",
    "    interpreter_quant.get_tensor(\n",
    "        interpreter_quant.get_output_details()[0][\"index\"]\n",
    "    ).flatten()\n",
    ")\n",
    "\n",
    "interpreter_quant.set_tensor(\n",
    "    interpreter_quant.get_input_details()[0][\"index\"],\n",
    "    tf.expand_dims(img_quant_float, 0),\n",
    ")\n",
    "interpreter_quant.invoke()\n",
    "print(\"Output with uncorrupted image:\")\n",
    "print(\n",
    "    interpreter_quant.get_tensor(\n",
    "        interpreter_quant.get_output_details()[0][\"index\"]\n",
    "    ).flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve weights and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_quant = interpreter_quant.get_tensor(3)\n",
    "weights_float = interpreter_float.get_tensor(3)\n",
    "weights_quantization = interpreter_quant.get_tensor_details()[3][\"quantization\"]\n",
    "\n",
    "weights_quant_diff = np.abs(\n",
    "    np.float32(weights_quant) - weights_float / weights_quantization[0]\n",
    ")\n",
    "weights_rel_err = norm(weights_quant_diff) / norm(np.float32(weights_quant))\n",
    "print(\n",
    "    \"Mean relative error between quantized and float weights: {:.4%}\".format(\n",
    "        weights_rel_err\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights_quant.reshape(-1, 28, 28)\n",
    "plt.figure(figsize=(16, 7))\n",
    "for j in range(10):\n",
    "    plt.subplot(2, 5, j + 1)\n",
    "    plt.imshow(w[j, :, :], vmin=-128, vmax=127)\n",
    "    plt.title(\"Digit {}\".format(j))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of weight quantization errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights_quant_diff.reshape(-1, 28, 28)\n",
    "plt.figure(figsize=(16, 1))\n",
    "for j in range(10):\n",
    "    plt.subplot(1, 10, j + 1)\n",
    "    plt.hist(w[j, :, :].reshape(-1))\n",
    "    plt.title(\"Digit {}\".format(j))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve biases and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_quant = interpreter_quant.get_tensor(4)\n",
    "bias_float = interpreter_float.get_tensor(4)\n",
    "bias_quantization = interpreter_quant.get_tensor_details()[4][\"quantization\"]\n",
    "\n",
    "bias_quant_diff = np.abs(\n",
    "    np.float32(bias_quant) - bias_quantization[1] - bias_float / bias_quantization[0]\n",
    ")\n",
    "bias_rel_err = norm(bias_quant_diff) / norm(np.float32(bias_quant))\n",
    "print(\n",
    "    \"Mean relative error between quantized and float matmul bieses: {:.4%}\".format(\n",
    "        bias_rel_err\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve preactivations and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the tensor dense/BiasAdd is actually a (pre)activation, not a bias\n",
    "preact_quant = interpreter_quant.get_tensor(2)\n",
    "preact_float = interpreter_float.get_tensor(2)\n",
    "preact_quantization = interpreter_quant.get_tensor_details()[2][\"quantization\"]\n",
    "\n",
    "preact_quant_diff = np.abs(\n",
    "    np.float32(preact_quant)\n",
    "    - preact_quantization[1]\n",
    "    - preact_float / preact_quantization[0]\n",
    ")\n",
    "preact_rel_err = norm(preact_quant_diff) / norm(np.float32(preact_quant))\n",
    "print(\n",
    "    \"Mean relative error between quantized and float preactivations: {:.4%}\".format(\n",
    "        preact_rel_err\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve outputs and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_float = interpreter_float.get_tensor(\n",
    "    interpreter_float.get_output_details()[0][\"index\"]\n",
    ")\n",
    "output_quant_float = interpreter_quant.get_tensor(\n",
    "    interpreter_quant.get_output_details()[0][\"index\"]\n",
    ")\n",
    "output_quant_int8 = interpreter_quant.get_tensor(0)\n",
    "output_quantization = interpreter_quant.get_tensor_details()[0][\"quantization\"]\n",
    "\n",
    "output_quant_diff = np.abs(\n",
    "    np.float32(output_quant_int8)\n",
    "    - output_quantization[1]\n",
    "    - output_float / output_quantization[0]\n",
    ")\n",
    "output_rel_err = norm(output_quant_diff) / norm(np.float32(output_quant_int8))\n",
    "print(\n",
    "    \"Mean relative error between quantized and float outputs: {:.4%}\".format(\n",
    "        output_rel_err\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreter reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float interpreter\n",
    "rec_preact_float = np.matmul(weights_float, img_float.flatten()) + bias_float\n",
    "rec_out_float = tf.math.softmax(rec_preact_float).numpy()\n",
    "\n",
    "with np.printoptions(formatter={\"float\": \"{:.6e}\".format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\n",
    "        \"Relative error: {:.6e}\".format(\n",
    "            norm(rec_out_float - output_float.flatten()) / norm(output_float.flatten())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights converted to float and float input (from quant model), compared quant output\n",
    "rec_preact_float2 = (\n",
    "    np.matmul(\n",
    "        np.float32(weights_quant) * weights_quantization[0], img_quant_float.flatten()\n",
    "    )\n",
    "    + bias_quant * bias_quantization[0]\n",
    ")\n",
    "\n",
    "rec_out_float2 = tf.math.softmax(rec_preact_float2).numpy()\n",
    "with np.printoptions(formatter={\"float\": \"{:.6e}\".format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float2))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\n",
    "        \"Relative error: {:.6e}\".format(\n",
    "            norm(rec_out_float2 - output_float.flatten()) / norm(output_float.flatten())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights converted to float and int input converted to float\n",
    "# NOTE: because of the above bug, float->int8->float converted image is used\n",
    "rec_preact_float3 = (\n",
    "    np.matmul(\n",
    "        np.float32(weights_quant) * weights_quantization[0],\n",
    "        (np.float32(img_quant_float_int8) - img_quantization[1]).flatten()\n",
    "        * img_quantization[0],\n",
    "    )\n",
    "    + bias_quant * bias_quantization[0]\n",
    ")\n",
    "rec_out_float3 = tf.math.softmax(rec_preact_float3).numpy()\n",
    "\n",
    "with np.printoptions(formatter={\"float\": \"{:.6e}\".format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float3))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\n",
    "        \"Relative error: {:.6e}\".format(\n",
    "            norm(rec_out_float3 - output_float.flatten()) / norm(output_float.flatten())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights and int input, using 32 bit accumulation and 32 bit bias\n",
    "rec_preact_int32 = (\n",
    "    np.matmul(np.int32(weights_quant), np.int32(img_quant_float_int8).flatten())\n",
    "    - np.matmul(\n",
    "        np.int32(weights_quant),\n",
    "        np.int32(img_quantization[1] * np.ones(img_quant_float_int8.size)),\n",
    "    )\n",
    "    + bias_quant\n",
    ")\n",
    "rec_out_int = tf.math.softmax(rec_preact_int32 * bias_quantization[0]).numpy()\n",
    "\n",
    "with np.printoptions(formatter={\"float\": \"{:.6e}\".format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_int))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\n",
    "        \"Relative error: {:.6e}\".format(\n",
    "            norm(rec_out_int - output_float.flatten()) / norm(output_float.flatten())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XS3 emulation and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XS3VPU import XS3VPU\n",
    "\n",
    "\n",
    "def compute_chunk(vpu, W, x, W_start, W_step, x_start):\n",
    "    # ~ 17 instructions\n",
    "    vpu.VLDC(x[x_start : x_start + vpu.ve])\n",
    "    rw = W_start\n",
    "    for _ in range(vpu.acc_period):  # unroll in asm\n",
    "        vpu.VLMACCR(W[rw : rw + vpu.ve])\n",
    "        rw += W_step\n",
    "\n",
    "\n",
    "def compute_tile(vpu, W, x, N_chunks, W_start, W_step, W_chunk_step, x_start, x_step):\n",
    "    # ~ N_chunks * (17 + 2) + 5\n",
    "    rx = x_start\n",
    "    rw = W_start\n",
    "    for _ in range(N_chunks):\n",
    "        compute_chunk(vpu, W, x, W_start=rw, W_step=W_step, x_start=rx)\n",
    "        rx += x_step\n",
    "        rw += W_chunk_step\n",
    "\n",
    "\n",
    "def XS3_matmul(vpu, W, x, y, N_bands, N_chunks):\n",
    "    # ~ N_bands * (N_chunks * (17 + 2) + 5 + 8) + 5\n",
    "    rw = 0\n",
    "    ry = 0\n",
    "    for _ in range(N_bands):\n",
    "        vpu.VCLRDR()  # TODO add bias loading\n",
    "        compute_tile(\n",
    "            vpu,\n",
    "            W,\n",
    "            x,\n",
    "            N_chunks,\n",
    "            W_start=rw,\n",
    "            W_step=N_chunks * vpu.ve,\n",
    "            W_chunk_step=vpu.ve,\n",
    "            x_start=0,\n",
    "            x_step=vpu.ve,\n",
    "        )\n",
    "        y[ry : ry + vpu.acc_period] = vpu._combine_vD_vR()  # VLSAT, VPOS, VSTRPV\n",
    "        rw += vpu.acc_period * N_chunks * vpu.ve\n",
    "        ry += vpu.acc_period\n",
    "\n",
    "\n",
    "def XS3_fc_forward_int32(W, b, x):\n",
    "    vpu = XS3VPU(bpe=8)\n",
    "    y = np.zeros((16,), dtype=np.int32).flatten()\n",
    "    XS3_matmul(vpu, W, x, y, N_bands=1, N_chunks=800 // vpu.ve)\n",
    "    return y + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad0 = 16 - weights_quant.shape[0]\n",
    "pad1 = weights_quant.shape[1] - 32 * (weights_quant.shape[1] // 32)\n",
    "weights_xs3 = np.pad(weights_quant, pad_width=[(0, pad0), (0, pad1)])\n",
    "weights_xs3 = np.flipud(weights_xs3).flatten()\n",
    "\n",
    "data_xs3 = np.pad(img_quant_float_int8.flatten(), pad_width=[(0, pad1)])\n",
    "\n",
    "bias_xs3 = bias_quant - np.matmul(\n",
    "    np.int32(weights_quant),\n",
    "    np.int32(img_quantization[1] * np.ones(img_quant_float_int8.size)),\n",
    ")\n",
    "bias_xs3 = np.pad(bias_xs3, pad_width=[(0, pad0)])\n",
    "\n",
    "y_xs3 = XS3_fc_forward_int32(weights_xs3, bias_xs3, data_xs3)\n",
    "\n",
    "preact_xs3_int32 = y_xs3[:-pad0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"int32 preactivation values produced by XS3 emulation (without offset):\")\n",
    "print(preact_xs3_int32)\n",
    "print(\"int32 preactivation values produced by int32 accumulation:\")\n",
    "print(rec_preact_int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to float preactivation\n",
    "rec_preact_xs3 = preact_xs3_int32 * bias_quantization[0]\n",
    "\n",
    "with np.printoptions(formatter={\"float\": \"{:.6e}\".format}):\n",
    "    print(\"Reconstructed preactivation (xs3):\\n{}\".format(rec_preact_xs3))\n",
    "    print(\"Original preactivation:\\n{}\".format(preact_float.flatten()))\n",
    "    print(\n",
    "        \"Relative error: {:.6e}\".format(\n",
    "            norm(rec_preact_xs3 - preact_float.flatten()) / norm(preact_float.flatten())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating final bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sat_to_16(a):\n",
    "    return np.int16(np.round(np.clip(a, -(2**15) + 1, 2**15 - 1)))\n",
    "\n",
    "\n",
    "def sat_to_8(a):\n",
    "    return np.int8(np.round(np.clip(a, -(2**7) + 1, 2**7 - 1)))\n",
    "\n",
    "\n",
    "def XS3_fc_forward_int8(W, b, x, rshift, scale):\n",
    "    y = XS3_fc_forward_int32(W, b, x)\n",
    "    preact_xs3_int32_offset = y[:-pad0]\n",
    "    preact_xs3_int32_vlsat = sat_to_16(preact_xs3_int32_offset / 2**rshift)\n",
    "    preact_xs3_int32_vlmul = sat_to_16(preact_xs3_int32_vlsat * scale / 2**14)\n",
    "    return sat_to_8(preact_xs3_int32_vlmul / 2**7)  # this is what VDEPTH8 would do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final bias is calculated here\n",
    "# this includes the output offset, so that fused activations are already applied\n",
    "bias_scale = bias_quantization[0]\n",
    "output_scale, output_zero_point = preact_quantization\n",
    "multiplier = bias_scale / output_scale\n",
    "\n",
    "rshift = -np.ceil(np.log2(multiplier)) + 1\n",
    "scale = np.round(2**14 * (multiplier * 2**rshift))\n",
    "if scale == 2**15:\n",
    "    rshift -= 1\n",
    "    scale /= 2\n",
    "rshift -= 7\n",
    "\n",
    "bias_xs3_offset = bias_xs3 + np.int32(output_zero_point / multiplier)\n",
    "\n",
    "preact_xs3_int8 = XS3_fc_forward_int8(\n",
    "    weights_xs3, bias_xs3_offset, data_xs3, rshift, scale\n",
    ")\n",
    "\n",
    "print(\"int8 preactivation values using xs3 emulation:\")\n",
    "print(preact_xs3_int8)\n",
    "print(\"int8 preactivation values produced by tflite:\")\n",
    "print(preact_quant[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of the XS3 emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def eval_pred(args):\n",
    "    j, im = args\n",
    "    data_xs3 = np.pad(\n",
    "        np.int8(im + img_quantization[1]).flatten(), pad_width=[(0, pad1)]\n",
    "    )\n",
    "    preact_xs3_int8 = XS3_fc_forward_int8(\n",
    "        weights_xs3, bias_xs3_offset, data_xs3, rshift, scale\n",
    "    )\n",
    "    if (j + 1) % 10 == 0:\n",
    "        print(\"{:6d}/10000\".format(j + 1))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return np.argmax(preact_xs3_int8)\n",
    "\n",
    "\n",
    "predictions_xs3 = np.zeros(predictions.shape, dtype=np.int64)\n",
    "p = Pool(10)\n",
    "predictions_xs3 = p.map(eval_pred, enumerate(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = tf.metrics.Accuracy()\n",
    "print(\"Accuracy of models:\")\n",
    "print(\"# Original keras model:   {:.2%}\".format(acc(test_labels, predictions).numpy()))\n",
    "print(\n",
    "    \"# Float TFLite model:     {:.2%}\".format(\n",
    "        acc(test_labels, predictions_float).numpy()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"# Quantized TFLite model: {:.2%}\".format(\n",
    "        acc(test_labels, predictions_quant).numpy()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"# Emulated XS3 model:     {:.2%}\".format(acc(test_labels, predictions_xs3).numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert tflite model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_utils import load_tflite_as_json, save_json_as_tflite\n",
    "from tflite2xcore_utils import (\n",
    "    clean_unused_opcodes,\n",
    "    clean_unused_tensors,\n",
    "    clean_unused_buffers,\n",
    ")\n",
    "from tflite2xcore_graph_conv import remove_float_inputs_outputs\n",
    "\n",
    "model_quant_stripped_file = \"models/model_quant_stripped.tflite\"\n",
    "\n",
    "json_model = load_tflite_as_json(model_quant_file)\n",
    "remove_float_inputs_outputs(json_model)\n",
    "clean_unused_opcodes(json_model)\n",
    "clean_unused_tensors(json_model)\n",
    "clean_unused_buffers(json_model)\n",
    "save_json_as_tflite(json_model, model_quant_stripped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
