{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Converting to TensorFlow Lite using pytorch_to_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to convert mobilenet_v2 from a pytorch model into a quantized TensorFlow Lite model. First, use the python library `pytorch2keras` to convert the model into a Keras model, then follow the usual steps to export from Keras as a quantised int8 tflite model.\n",
    "\n",
    "Ensure that you have installed Python 3.8 and have the installed `../requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original libraries are unable to convert Relu6 operations\n",
    "!{sys.executable} -m pip install https://github.com/xmos/onnx2keras/archive/refs/heads/fix/relu6.zip\n",
    "!{sys.executable} -m pip install https://github.com/xmos/pytorch2keras/archive/refs/heads/fix/relu6.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import io, os, shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflite\n",
    "from pytorch2keras import pytorch_to_keras\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PyTorch Model\n",
    "For this example, we use mobilenet_v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_model = torch.hub.load(\n",
    "    \"pytorch/vision:v0.10.0\", \"mobilenet_v2\", pretrained=True\n",
    ")\n",
    "# Switch the model to eval mode\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Inference on PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets run inference on the PyTorch model directly, just to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download an image to test against\n",
    "import urllib\n",
    "\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try:\n",
    "    urllib.URLopener().retrieve(url, filename)\n",
    "except:\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "import requests\n",
    "\n",
    "# Download Image Labels\n",
    "resp = requests.get(\n",
    "    \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    ")\n",
    "# Read the categories\n",
    "categories = [s.strip() for s in resp.text.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will test and train with these params\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Open testing image\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(height),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Note Pytorch is BCHW\n",
    "input_tensor = preprocess(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pytorch_model(input_batch)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# Show top categories per image\n",
    "vals, idxs = torch.topk(probabilities, 5)\n",
    "pytorch_results = [\n",
    "    (categories[idx], prob) for (idx, prob) in zip(idxs.tolist(), vals.tolist())\n",
    "]\n",
    "for cat, prob in pytorch_results:\n",
    "    print(cat, \":\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pytorch_to_keras_model(pytorch_model, input_shape) -> tf.keras.Model:\n",
    "    input_np = np.random.uniform(0, 1, tuple([1]) + input_shape)\n",
    "    input_var = Variable(torch.FloatTensor(input_np))\n",
    "\n",
    "    return pytorch_to_keras(\n",
    "        pytorch_model,\n",
    "        input_var,\n",
    "        [input_shape],\n",
    "        verbose=True,\n",
    "        name_policy=\"renumerate\",\n",
    "        change_ordering=True,  # change channel_first to channel_last\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_model = pytorch_to_keras_model(pytorch_model, input_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check keras conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(xs):\n",
    "    return np.exp(xs) / sum(np.exp(xs))\n",
    "\n",
    "\n",
    "# transpose the input_batch into BHWC order for tensorflow\n",
    "tf_input_data = np.transpose(input_batch.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "keras_output_data = keras_model(tf_input_data)\n",
    "\n",
    "probs = keras_output_data[0]\n",
    "data = zip(range(len(probs)), probs)\n",
    "keras_results = [\n",
    "    (categories[idx], prob)\n",
    "    for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]\n",
    "]\n",
    "for cat, prob in keras_results:\n",
    "    print(cat, \":\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convert to tflite\n",
    "\n",
    "We will still feed the data into the model in float32 format for convinence but the internals of the model will be int8. This will require representitive data but as we interface in float32 we can use the pytorch preprocessing. \n",
    "\n",
    "This conversion follows the method from [keras_to_xcore.ipynb](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Representative Dataset\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb) for more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds = (\n",
    "    tfds.load(\"imagenette\", split=\"train\", as_supervised=True, shuffle_files=True)\n",
    "    .shuffle(1000)\n",
    "    .batch(1)\n",
    "    .prefetch(10)\n",
    "    .take(1000)\n",
    ")\n",
    "\n",
    "\n",
    "# Iterate over the sampled images and preprocess them\n",
    "def representative_dataset():\n",
    "    for image, _ in ds:\n",
    "        pil_img = tf.keras.utils.array_to_img(image[0])\n",
    "        pytorch_batch = preprocess(pil_img).unsqueeze(0)\n",
    "        tf_batch = np.transpose(pytorch_batch.numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = \"mobilenet_v2.tflite\"\n",
    "with open(tflite_int8_model_path, \"wb\") as f:\n",
    "    f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "# Convert PyTorch Input Tensor into Numpy Matrix and Reshape for TensorFlow\n",
    "tfl_interpreter.set_tensor(tfl_input_details[0][\"index\"], tf_input_data)\n",
    "tfl_interpreter.invoke()\n",
    "\n",
    "tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0][\"index\"])\n",
    "\n",
    "probs = softmax(tfl_output_data[0])\n",
    "data = zip(range(len(probs)), probs)\n",
    "tfl_int8_results = [\n",
    "    (categories[idx], prob)\n",
    "    for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]\n",
    "]\n",
    "for cat, prob in tfl_int8_results:\n",
    "    print(cat, \":\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Model\n",
    "\n",
    "### Check Operator Counts\n",
    "\n",
    "Lets take a look at the operator counts inside the converted model. This uses a helper function defined in `../utils`, but this step is not necessary to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.print_operator_counts(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Let's compare the accuracy of the converted model to the original PyTorch model.\n",
    "\n",
    "To do this, we take a large sampel from imagenet_v2 and compare the classifications returned by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# load dataset\n",
    "ds, info = tfds.load(\n",
    "    \"imagenet_v2\", split=\"test\", with_info=True, as_supervised=True, shuffle_files=False\n",
    ")\n",
    "ds = ds.shuffle(100, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_tflite(top_n: int = 1, samples=1000, verbose=False) -> float:\n",
    "    if top_n < 1 or n > 1000:\n",
    "        raise ValueError\n",
    "\n",
    "    # take subset of dataset\n",
    "    selection = ds.prefetch(10).take(samples)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for image, label in selection:\n",
    "        trueCatIdx = tf.get_static_value(label)\n",
    "        # convert to PIL.Image\n",
    "        img = tf.keras.utils.array_to_img(image)\n",
    "\n",
    "        # preprocess using PyTorch functions then convert back into Tf.Tensor\n",
    "        pytorch_batch = preprocess(img).unsqueeze(0)\n",
    "        tf_batch = np.transpose(pytorch_batch.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "        # use same tflite interpreter as before\n",
    "        tfl_interpreter.set_tensor(tfl_input_details[0][\"index\"], tf_batch)\n",
    "        tfl_interpreter.invoke()\n",
    "\n",
    "        output = tfl_interpreter.get_tensor(tfl_output_details[0][\"index\"])\n",
    "\n",
    "        # Sort into List[Tuple[index, confidence]] ordered by confidence (descending)\n",
    "        data = sorted(\n",
    "            zip(range(len(output[0])), output[0]), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "\n",
    "        top_n_results: List[int] = [idx for (idx, _) in data[:top_n]]\n",
    "\n",
    "        if trueCatIdx in top_n_results:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            incorrect = incorrect + 1\n",
    "            if verbose:\n",
    "                print(\"--incorrect--\")\n",
    "                print(f\"True Category: {categories[trueCatIdx]}({trueCatIdx})\")\n",
    "                print(\n",
    "                    [\n",
    "                        f\"Top-{top_n} categories: {categories[idx]}({idx})\"\n",
    "                        for idx in top_n_results\n",
    "                    ]\n",
    "                )\n",
    "                display(img)\n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    print(\n",
    "        f\"Top-{top_n} accuracy (TFLite Model): {accuracy * 100}% ({correct}/{correct+incorrect})\"\n",
    "    )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_torch(top_n: int = 1, samples=1000, verbose=False):\n",
    "    if top_n < 1 or n > 1000:\n",
    "        raise ValueError\n",
    "\n",
    "    # take subset of dataset\n",
    "    selection = ds.prefetch(10).take(samples)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for image, label in selection:\n",
    "        trueCatIdx = tf.get_static_value(label)\n",
    "\n",
    "        # convert to PIL.Image\n",
    "        img = tf.keras.utils.array_to_img(image)\n",
    "        input_batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = pytorch_model(input_batch)\n",
    "\n",
    "        # Show top categories per image\n",
    "        vals, idxs = torch.topk(output[0], top_n)\n",
    "\n",
    "        if trueCatIdx in idxs:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            incorrect = incorrect + 1\n",
    "            if verbose:\n",
    "                print(\"--incorrect--\")\n",
    "                print(f\"True Category: {categories[trueCatIdx]}({trueCatIdx})\")\n",
    "                print(\n",
    "                    [\n",
    "                        f\"Top-{top_n} categories: {categories[idx]}({idx})\"\n",
    "                        for idx in top_n_results\n",
    "                    ]\n",
    "                )\n",
    "                display(img)\n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    print(\n",
    "        f\"Top-{top_n} accuracy (PyTorch Model): {accuracy * 100}% ({correct}/{correct+incorrect})\"\n",
    "    )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = 500\n",
    "for n in range(5):\n",
    "    accuracy_torch(n + 1, samples)\n",
    "    accuracy_tflite(n + 1, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
