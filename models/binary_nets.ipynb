{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:65% !important; }</style>\"))\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import larq\n",
    "import larq_zoo\n",
    "\n",
    "from larq.models import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = larq_zoo.BinaryResNetE18()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, 'tmp.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General notes\n",
    "Layers with float32 weights will be approximated assuming int8 quantization. Biases and output shifts/scales are approximated according to standard XS3 practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_shallowin_cnt(output_height, output_width, C_out, K_h):\n",
    "    num_out_pixels = output_height * output_width\n",
    "    return num_out_pixels * ((C_out//16) * (K_h * ((17 + 2) + 5 + 2) + 5 + 8) + 5)\n",
    "\n",
    "def conv2d_deepin_cnt(output_height, output_width, C_in, C_out, K_h, K_w):\n",
    "    num_out_pixels = output_height * output_width\n",
    "    return num_out_pixels * ((C_out//16) * (K_h * (K_w*(C_in//32) * (17 + 2) + 5 + 2) + 5 + 8) + 5)\n",
    "\n",
    "def max_pool_3x3_cnt(output_height, output_width, depth, price_per_comparison=44):\n",
    "    # price_per_comparison is based on results of first benchmarking\n",
    "    # a single output requires 9 input pixels\n",
    "    # 9 vectors mean 4 + 2 + 1 + 1 comparisons per output pixel\n",
    "    return output_height * output_width * (depth//32) * (4+2+1+1) * price_per_comparison\n",
    "\n",
    "def add_cnt(height, width, depth, price_per_addition=9):\n",
    "    # price_per_addition is based on the following asm prototype for vector add:\n",
    "    # (assume vC is initialized with a vector of ones)\n",
    "    # VLCLRDR\n",
    "    # VSETC to 8 bit mode\n",
    "    # VLMACC a\n",
    "    # VLMACC b\n",
    "    # VSETC to 16 bit mode\n",
    "    # VLMUL to scale\n",
    "    # VDEPTH8\n",
    "    # VSTRPV lower half\n",
    "    # loop\n",
    "    return height * width * (depth//16) * price_per_addition\n",
    "\n",
    "def binarize_cnt(height, width, depth, price_per_vector=5):\n",
    "    # price_per_vector is based on the following asm prototype:\n",
    "    # VLDR a\n",
    "    # VLSUB zero_point_vector\n",
    "    # VDETPH1\n",
    "    # VSTRPV\n",
    "    # loop\n",
    "    return height * width * (depth//32) * price_per_vector\n",
    "\n",
    "def ave_pool_2x2_cnt(output_height, output_width, depth, price_per_output_pixel=11):\n",
    "    # price_per_output_pixel is based on the following asm prototype for 4-element averaging:\n",
    "    # (assume vC is initialized with a vector of ones)\n",
    "    # VLCLRDR\n",
    "    # VSETC to 8 bit mode\n",
    "    # VLMACC a\n",
    "    # VLMACC b\n",
    "    # VLMACC c\n",
    "    # VLMACC d\n",
    "    # VSETC to 16 bit mode\n",
    "    # VLMUL/VLASHR to scale/divide\n",
    "    # VDEPTH8\n",
    "    # VSTRPV lower half\n",
    "    # loop\n",
    "    return output_height * output_width * (depth//16) * price_per_output_pixel\n",
    "\n",
    "def global_ave_pool_cnt(size, depth):\n",
    "    # estimate is based on the following prototype for averange pooling a single input channel group:\n",
    "    # (assume vC is initialized with a vector of ones)\n",
    "    # VLCLRDR\n",
    "    # VSETC to 8 bit mode\n",
    "    # loop size times\n",
    "    #     VLMACC\n",
    "    # VSETC to 16 bit mode\n",
    "    # VLMUL/VLASHR to scale/divide\n",
    "    # VDEPTH8\n",
    "    # VSTRPV lower half\n",
    "    # loop (next channel group)\n",
    "    return (depth//16) * (size * 2 + 7)\n",
    "\n",
    "def mem_load_cnt(size_in_bytes):\n",
    "    mem_load_speed = 800e6 / 800e6 * 5  # DDR access is about 800MB/s, this is per instruction\n",
    "    return int(size_in_bytes / mem_load_speed)\n",
    "\n",
    "def fc_cnt(N_in, N_out):\n",
    "    # assume the standard fully connected strategy\n",
    "    return (N_out // 16) * ((N_in // 32) * (17 + 2) + 5 + 8) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mem(mem):\n",
    "    print(\"Data memory fooptrint:\")\n",
    "    print(f\"  Input:            {mem[0] / 1024: >6.1f} KB\")\n",
    "    print(f\"  Output:           {mem[1] / 1024: >6.1f} KB\")\n",
    "    print(f\"  Intermediate:     {mem[2] / 1024: >6.1f} KB\")\n",
    "    print(\"Layer parameters:\")\n",
    "    print(f\"  Weights:          {mem[3] / 1024: >6.1f} KB\")\n",
    "    print(f\"  Bias/shift/scale  {mem[4] / 1024: >6.1f} KB\")\n",
    "    if len(mem) > 5:\n",
    "        print(\"Next layer's parameters:\")\n",
    "        print(f\"  Weights:          {mem[5] / 1024: >6.1f} KB\")\n",
    "        print(f\"  Bias/shift/scale  {mem[6] / 1024: >6.1f} KB\")\n",
    "    print(f\"TOTAL:              {sum(mem) / 1024: >6.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem\n",
    "This is the sequence of operators before the first residual connection.\n",
    "Input is 224x224x3 and output is 56x56x64.\n",
    "The convolution, the activation, and the two batchnorms can be fused (second batchnorm normally commutes with maxpool).\n",
    "\n",
    "NOTE: conv2d is 7x7 with 2x2 strides, without biases.\n",
    "Since input is shallow, kernel tensor is zero padded for fast access (normal XS3 practice).\n",
    "\n",
    "NOTE: maxpool2d is 3x3 with 2x2 strides.\n",
    "\n",
    "NOTE: The intermediate ouput of the conv2s does not fit in memory, so a special implementation will be needed.\n",
    "(The pooling needs to happen in parallel with the conv2d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_mem = [224**2 * 3 * 4/3, 56**2 * 64, 3 * 3 * 64,\n",
    "            9408 * 8/7 * 4/3 , 64 * 2 * 4,\n",
    "            36864 * 4/3 / 8, 64 * 2 * 4]\n",
    "print_mem(stem_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_cnt = [conv2d_shallowin_cnt(output_height=112, output_width=112, C_out=64, K_h=7),\n",
    "            max_pool_3x3_cnt(output_height=56, output_width=56, depth=64)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  7x7 conv2d:  {stem_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 maxpool: {stem_cnt[1] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st residual block\n",
    "\n",
    "There will be 4 of these blocks, each identical except the last one, which needs to preload more weights.\n",
    "This changes the memory footprint for the fourth layer, but not the computational cost.\n",
    "This larger memory footprint is shown below.\n",
    "\n",
    "NOTE: the binary conv2d is 3x3 with 1x1 strides, without biases.\n",
    "Since a single row of the kernel weights is only $3*64 < 256$ bits wide, we apply some zero padding in the width direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_1_mem = [56**2 * 64, 56**2 * 64, 56**2 * 64 / 8,\n",
    "                   36864 * 4/3 / 8, 64 * 2 * 4,\n",
    "                   8192, 128 * 2 * 4]\n",
    "print_mem(res_block_1_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_1_cnt = [binarize_cnt(height=56, width=56, depth=64),\n",
    "                   conv2d_shallowin_cnt(output_height=56, output_width=56, C_out=64, K_h=3),\n",
    "                   add_cnt(height=56, width=56, depth=64)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  binarization:  {res_block_1_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 conv2d:    {res_block_1_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  addition:      {res_block_1_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st downsampling block, pooling branch\n",
    "\n",
    "This can be executed first, and its result offloaded to flash temporarily, then load back when doing the addition with the result of the conv branch.\n",
    "\n",
    "NOTE: the only intermediate is the output of the average pooling.\n",
    "\n",
    "NOTE: the conv2d kernel is 1x1, and deepin/deepout strategy can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_1_pool_mem = [56**2 * 64, 28**2 * 128, 28**2 * 64,\n",
    "                       8192, 128 * 2 * 4,\n",
    "                       73728 * 4/3 / 8, 128 * 2 * 4]\n",
    "print_mem(ds_block_1_pool_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_1_pool_cnt = [ave_pool_2x2_cnt(output_height=28, output_width=28, depth=64),\n",
    "                       conv2d_deepin_cnt(output_height=28, output_width=28, C_in=64, C_out=128, K_h=1, K_w=1)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  Ave pooling:   {ds_block_1_pool_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  1x1 conv2d:    {ds_block_1_pool_cnt[1] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st downsampling block, binarized conv2d branch\n",
    "\n",
    "NOTE: the binary conv2d is 3x3 with 2x2 strides, without biases.\n",
    "Since a single row of the kernel weights is only $3*64 < 256$ bits wide, we apply some zero padding in the width direction.\n",
    "\n",
    "NOTE: the only intermediate is the output of the average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_1_conv_mem = [56**2 * 64, 28**2 * 128, 56**2 * 64 / 8,\n",
    "                       73728 * 4/3 / 8, 128 * 2 * 4,\n",
    "                       147456 * 4/3 / 8, 128 * 2 * 4]\n",
    "print_mem(ds_block_1_conv_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_1_conv_cnt = [binarize_cnt(height=56, width=56, depth=64),\n",
    "                       conv2d_shallowin_cnt(output_height=28, output_width=28, C_out=128, K_h=3),\n",
    "                       add_cnt(height=28, width=28, depth=128)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  binarization:  {ds_block_1_conv_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 conv2d:    {ds_block_1_conv_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  addition:      {ds_block_1_conv_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd residual block\n",
    "\n",
    "There will be 3 of these blocks, each identical except the last one, which needs to preload more weights.\n",
    "This changes the memory footprint for the third layer, but not the computational cost.\n",
    "This larger memory footprint is shown below.\n",
    "\n",
    "NOTE: the binary conv2d is 3x3 with 1x1 strides, without biases.\n",
    "Since a single row of the kernel weights is only $3*128$ bits wide, we apply 128 bits of zero padding in the width direction and compute two horizontally adjacent pixels at a time.\n",
    "The 3x3 kernel will therefore be modeled as a 6 x 2 shallowing convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_2_mem = [28**2 * 128, 28**2 * 128, 28**2 * 128 / 8,\n",
    "                   147456 * 4/3 / 8, 128 * 2 * 4,\n",
    "                   32768, 256 * 2 * 4]\n",
    "print_mem(res_block_2_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_2_cnt = [binarize_cnt(height=28, width=28, depth=128),\n",
    "                   conv2d_shallowin_cnt(output_height=28, output_width=28, C_out=128, K_h=2*3),\n",
    "                   add_cnt(height=28, width=28, depth=128)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  binarization:  {res_block_2_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 conv2d:    {res_block_2_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  addition:      {res_block_2_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd downsampling block, pooling branch\n",
    "\n",
    "This can be executed first, and its result offloaded to flash temporarily, then load back when doing the addition with the result of the conv branch.\n",
    "\n",
    "NOTE: the only intermediate is the output of the average pooling.\n",
    "\n",
    "NOTE: the conv2d kernel is 1x1, and deepin/deepout strategy can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_2_pool_mem = [28**2 * 128, 14**2 * 256, 14**2 * 128,\n",
    "                       32768, 256 * 2 * 4,\n",
    "                       294912 * 4/3 / 8, 256 * 2 * 4]\n",
    "print_mem(ds_block_2_pool_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_2_pool_cnt = [ave_pool_2x2_cnt(output_height=14, output_width=14, depth=128),\n",
    "                       conv2d_deepin_cnt(output_height=14, output_width=14, C_in=128, C_out=256, K_h=1, K_w=1)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  Ave pooling:   {ds_block_2_pool_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  1x1 conv2d:    {ds_block_2_pool_cnt[1] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd downsampling block, binarized conv2d branch\n",
    "\n",
    "NOTE: the binary conv2d is 3x3 with 2x2 strides, without biases.\n",
    "Since a single row of the kernel weights is only $3*128$ bits wide, we apply 128 bits of zero padding in the width direction and compute two horizontally adjacent pixels at a time.\n",
    "The 3x3 kernel will therefore be modeled as a 6 x 2 shallowing convolution.\n",
    "\n",
    "NOTE: the only intermediate is the output of the average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_2_conv_mem = [28**2 * 128, 14**2 * 256, 28**2 * 128 / 8,\n",
    "                       294912 * 4/3 / 8, 256 * 2 * 4,\n",
    "                       589824 / 8, 256 * 2 * 4]\n",
    "print_mem(ds_block_2_conv_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_2_conv_cnt = [binarize_cnt(height=28, width=28, depth=128),\n",
    "                       conv2d_shallowin_cnt(output_height=14, output_width=14, C_out=256, K_h=6),\n",
    "                       add_cnt(height=14, width=14, depth=256)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  binarization:  {ds_block_2_conv_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 conv2d:    {ds_block_2_conv_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  addition:      {ds_block_2_conv_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd residual block\n",
    "\n",
    "There will be 3 of these blocks, each identical except the last one, which needs to preload more weights.\n",
    "This changes the memory footprint for the third layer, but not the computational cost.\n",
    "This larger memory footprint is shown below.\n",
    "\n",
    "NOTE: the binary conv2d is 3x3 with 1x1 strides, without biases. Since the number of input channels is $1 * 256$, the instruction count is modeled using deepin_deepout strategy with $1 * 32$ inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_3_mem = [14**2 * 256, 14**2 * 256, 14**2 * 256 / 8,\n",
    "                   589824 / 8, 256 * 2 * 4,\n",
    "                   131072, 512 * 2 * 4]\n",
    "print_mem(res_block_3_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_3_cnt = [binarize_cnt(height=14, width=14, depth=256),\n",
    "                   conv2d_deepin_cnt(output_height=14, output_width=14, C_in=32, C_out=256, K_h=3, K_w=3),\n",
    "                   add_cnt(height=14, width=14, depth=256)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  binarization:  {res_block_3_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 conv2d:    {res_block_3_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  addition:      {res_block_3_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd downsampling block, pooling branch\n",
    "\n",
    "This can be executed first, and its result offloaded to flash temporarily, then load back when doing the addition with the result of the conv branch.\n",
    "\n",
    "NOTE: the only intermediate is the output of the average pooling.\n",
    "\n",
    "NOTE: the conv2d kernel is 1x1, and deepin/deepout strategy can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_3_pool_mem = [14**2 * 256, 7**2 * 512, 7**2 * 256,\n",
    "                       131072, 512 * 2 * 4,\n",
    "                       1179648 / 8, 512 * 2 * 4]\n",
    "print_mem(ds_block_3_pool_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_3_pool_cnt = [ave_pool_2x2_cnt(output_height=7, output_width=7, depth=256),\n",
    "                       conv2d_deepin_cnt(output_height=7, output_width=7, C_in=256, C_out=512, K_h=1, K_w=1)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  Ave pooling:   {ds_block_3_pool_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  1x1 conv2d:    {ds_block_3_pool_cnt[1] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd downsampling block, binarized conv2d branch\n",
    "\n",
    "NOTE: the binary conv2d is 3x3 with 2x2 strides, without biases.\n",
    "Since the number of input channels is $1 ∗ 256$, the instruction count is modeled using deepin_deepout strategy with $1∗32$ inputs.\n",
    "\n",
    "NOTE: the only intermediate is the output of the average pooling.\n",
    "\n",
    "NOTE: too many weights in the next layer, so no pre-loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_3_conv_mem = [14**2 * 256, 7**2 * 512, 14**2 * 256 / 8,\n",
    "                       1179648 / 8, 512 * 2 * 4]\n",
    "print_mem(ds_block_3_conv_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_3_conv_cnt = [binarize_cnt(height=14, width=14, depth=256),\n",
    "                       conv2d_deepin_cnt(output_height=7, output_width=7, C_in=32, C_out=512, K_h=3, K_w=3),\n",
    "                       add_cnt(height=7, width=7, depth=512)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  binarization:  {ds_block_3_conv_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 conv2d:    {ds_block_3_conv_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  addition:      {ds_block_3_conv_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th residual block\n",
    "\n",
    "There will be 3 of these blocks, each identical.\n",
    "The last one will fuse the nonlinear activation that follows the last Add layer.\n",
    "\n",
    "NOTE: the binary conv2d is 3x3 with 1x1 strides, without biases.\n",
    "Since the number of input channels is $1 * 512$, the instruction count is modeled using deepin_deepout strategy with $2 * 32$ inputs.\n",
    "\n",
    "NOTE: too many weights in the next layer, so no pre-loading.\n",
    "The weight loading is estimated separately, since it cannot be parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_4_mem = [7**2 * 512, 7**2 * 512, 14**2 * 256 / 8,\n",
    "                   2359296 / 8, 512 * 2 * 4]\n",
    "print_mem(res_block_4_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_4_cnt = [binarize_cnt(height=7, width=7, depth=512),\n",
    "                   conv2d_deepin_cnt(output_height=7, output_width=7, C_in=64, C_out=512, K_h=3, K_w=3),\n",
    "                   add_cnt(height=7, width=7, depth=512)]\n",
    "res_block_4_load = [mem_load_cnt(2359296 // 8)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  weight load:   {res_block_4_load[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  binarization:  {res_block_4_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  3x3 conv2d:    {res_block_4_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  addition:      {res_block_4_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final layers\n",
    "\n",
    "We split the final fully connected layer into two parts, executing sequentially.\n",
    "No preloading of weights, so loading penalty is added.\n",
    "\n",
    "NOTE: The fully connected layer is approximated by two 512x512 layers.\n",
    "This will probably be faster, even with 12 zero padding channels.\n",
    "\n",
    "NOTE: The only intermediate is the output of the global average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mem = [7**2 * 512, 512, 1000,\n",
    "             500*512, 500 * 2 * 4]\n",
    "print_mem(final_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cnt = [global_ave_pool_cnt(7*7, 512),\n",
    "             fc_cnt(512, 512),\n",
    "             fc_cnt(512, 512)]\n",
    "final_load = [mem_load_cnt(512*512),\n",
    "              mem_load_cnt(512*512)]\n",
    "print(\"Instruction count estimates:\")\n",
    "print(f\"  global ave pooling: {final_cnt[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  weight load:        {final_load[0] * 1e-6: >1.3f} M\")\n",
    "print(f\"  FC first half:      {final_cnt[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  weight load:        {final_load[1] * 1e-6: >1.3f} M\")\n",
    "print(f\"  FC second half:     {final_cnt[2] * 1e-6: >1.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_list = [\n",
    "    stem_cnt,\n",
    "    res_block_1_cnt,\n",
    "    res_block_1_cnt,\n",
    "    res_block_1_cnt,\n",
    "    res_block_1_cnt,\n",
    "    ds_block_1_conv_cnt,\n",
    "    ds_block_1_pool_cnt,\n",
    "    res_block_2_cnt,\n",
    "    res_block_2_cnt,\n",
    "    res_block_2_cnt,\n",
    "    ds_block_2_conv_cnt,\n",
    "    ds_block_2_pool_cnt,\n",
    "    res_block_3_cnt,\n",
    "    res_block_3_cnt,\n",
    "    res_block_3_cnt,\n",
    "    ds_block_3_conv_cnt,\n",
    "    ds_block_3_pool_cnt,\n",
    "    res_block_4_cnt,\n",
    "    res_block_4_cnt,\n",
    "    res_block_4_cnt,\n",
    "    final_load,\n",
    "    final_cnt\n",
    "]\n",
    "cnt_sum = sum(d for c in cnt_list for d in c)\n",
    "\n",
    "load_list = [res_block_4_load, res_block_4_load, res_block_4_load, final_load]\n",
    "load_sum = sum(d for c in load_list for d in c)\n",
    "cnt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "IBUFFER_FACTOR = 5/4\n",
    "CLOCK_600, CLOCK_800 = 600e6, 800e6\n",
    "NOMINAL_MACS = 1.81e9\n",
    "\n",
    "total_cycles = 5 * (cnt_sum / NUM_CORES + load_sum) * IBUFFER_FACTOR\n",
    "total_time_800 = total_cycles / CLOCK_800\n",
    "print(f\"Total non-prefetch instructions: {cnt_sum * 1e-6: 3.2f}M\")\n",
    "print(f\"Total prefetch instructions:     {load_sum * 1e-6: 3.2f}M\")\n",
    "print(f\"Total clock cycles:              {total_cycles * 1e-6: 3.2f}M\")\n",
    "print(f\"Total time of execution @{CLOCK_800 * 1e-6:3.0f}MHz: {total_time_800 * 1e3: 3.2f}ms\")\n",
    "print(f\"Total MAC/s:                     {NOMINAL_MACS / total_time_800 * 1e-9: 3.2f}GMAC/s\")\n",
    "print(f\"Nominal binary MAC utilization:  {(NOMINAL_MACS / total_time_800) / (CLOCK_800*256): 3.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Proportion of input layer conv instructions: {cnt_list[0][0] / cnt_sum:.2%}\")\n",
    "print(f\"Proportion of input layer maxpool instructions: {cnt_list[0][1] / cnt_sum:.2%}\")\n",
    "print(f\"Proportion of binary instructions with suboptimal input channels: \"\n",
    "      f\"{sum(c[1] for c in (cnt_list[1:6] + cnt_list[7:11])) / cnt_sum:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WATTAGE = 500e-3\n",
    "\n",
    "print(f\"Energy efficiency of the chip:  {2 * NOMINAL_MACS / total_time_800 * 1e-9 / WATTAGE: 3.2f}GOp/s/W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_list = [\n",
    "    stem_mem,\n",
    "    res_block_1_mem,\n",
    "    res_block_1_mem,\n",
    "    res_block_1_mem,\n",
    "    res_block_1_mem,\n",
    "    ds_block_1_pool_mem,\n",
    "    ds_block_1_conv_mem,\n",
    "    res_block_2_mem,\n",
    "    res_block_2_mem,\n",
    "    res_block_2_mem,\n",
    "    ds_block_2_pool_mem,\n",
    "    ds_block_2_conv_mem,\n",
    "    res_block_3_mem,\n",
    "    res_block_3_mem,\n",
    "    res_block_3_mem,\n",
    "    ds_block_3_pool_mem,\n",
    "    ds_block_3_conv_mem,\n",
    "    res_block_4_mem,\n",
    "    res_block_4_mem,\n",
    "    res_block_4_mem,\n",
    "    final_mem,\n",
    "    final_mem\n",
    "]\n",
    "\n",
    "mem_sum = sum(d for c in mem_list for d in c[3:5])\n",
    "print(f\"Total memory for weights and parameters: {mem_sum / 2**20 :.2f}MB\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
