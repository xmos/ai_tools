{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c42ac65",
   "metadata": {},
   "source": [
    "# Converting PyTorch to TensorFlow Lite for xCORE Using ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models. We can convert from PyTorch to ONNX, then from ONNX to TensorFlow, then from TensorFlow to TensorFlow Lite, and finally, run it through xformer to optimise it for xCORE.\n",
    "\n",
    "Ensure that you have installed Python 3.8 and have the installed requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdecdb6-82c4-4dc3-be6c-524b017c2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90a93c",
   "metadata": {},
   "source": [
    "## Import PyTorch Model\n",
    "\n",
    "For this example, we use mobilenet_v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69af6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pytorch_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "# Switch the model to eval mode\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2033740",
   "metadata": {},
   "source": [
    "### Download the Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d609035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get(\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\")\n",
    "# Read the categories\n",
    "categories = [s.strip() for s in resp.text.splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2630c0",
   "metadata": {},
   "source": [
    "### Perform an infrence on the pytorch model\n",
    "Perform inference on the model to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10252ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image to test against\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test and train with these params\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb63c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Open testing image\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(height),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Note Pytorch is BCHW\n",
    "input_tensor = preprocess(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pytorch_model(input_batch)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# Show top categories per image\n",
    "vals, idxs = torch.topk(probabilities, 5)\n",
    "pytorch_results = [(categories[idx], prob) for (idx, prob) in zip(idxs.tolist(), vals.tolist())]\n",
    "for cat, prob in  pytorch_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fde0a",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for shape info for tracing the model during conversion\n",
    "sample_input = torch.rand((batch_size, channels, height, width))\n",
    "\n",
    "onnx_model_path = \"mobilenet_v2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    pytorch_model,\n",
    "    sample_input,\n",
    "    onnx_model_path,\n",
    "    input_names=['image'],\n",
    "    output_names=['probabilities']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6475e32",
   "metadata": {},
   "source": [
    "### Check the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d2393",
   "metadata": {},
   "source": [
    "### Check ONNX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime \n",
    "import numpy as np\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def softmax(xs):\n",
    "    return np.exp(xs)/sum(np.exp(xs))\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "input_batch_np = to_numpy(input_batch)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: input_batch_np}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# The input is still BCHW\n",
    "data = zip(range(len(ort_outs[0][0])), softmax(ort_outs[0][0]))\n",
    "\n",
    "onnx_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  onnx_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5ad56",
   "metadata": {},
   "source": [
    "### Convert ONNX to Keras\n",
    "We do this using the `onnx2tf` package: https://github.com/PINTO0309/onnx2tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2722c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model_path = 'mobilenet_v2.tf'\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_model_path,\n",
    "    output_folder_path=keras_model_path,\n",
    "    non_verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549092a",
   "metadata": {},
   "source": [
    "### Check the conversion to keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cdf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose the input_batch into BHWC order for tensorflow\n",
    "tf_input_data = np.transpose( input_batch.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "keras_output_data = keras_model(tf_input_data)\n",
    "\n",
    "probs = softmax(keras_output_data[0])\n",
    "data = zip(range(len(probs)), probs)\n",
    "keras_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  keras_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f3aa5",
   "metadata": {},
   "source": [
    "## Convert Keras to TFLite (float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b392bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_float_model_path = 'mobilenet_v2_float.tflite'\n",
    "with open(tflite_float_model_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66dc2b",
   "metadata": {},
   "source": [
    "### Check it Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_float_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "# Convert PyTorch Input Tensor into Numpy Matrix and Reshape for TensorFlow\n",
    "# (Pytorch model expects C x H x W but TF expects H x W x C)\n",
    "tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tf_input_data)\n",
    "tfl_interpreter.invoke()\n",
    "\n",
    "tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "probs = softmax(tfl_output_data[0])\n",
    "data = zip(range(len(probs)), probs)\n",
    "tfl_float32_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  tfl_float32_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497e02a",
   "metadata": {},
   "source": [
    "## Convert Keras to TFLite (int8)\n",
    "We will still feed the data into the model in float32 format for convinence but the internals of the model will be int8. This will require representitive data but as we interface in float32 we can use the pytorch preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1db68",
   "metadata": {},
   "source": [
    "### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb) for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a5c3e",
   "metadata": {},
   "source": [
    "#### Download & Extract Images from Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32503a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds = tfds.load('imagenette', split='train', as_supervised=True, shuffle_files=True).shuffle(1000).batch(1).prefetch(10).take(1000)\n",
    "\n",
    "# Iterate over the sampled images and preprocess them\n",
    "def representative_dataset():\n",
    "    for image, _ in ds:\n",
    "        pil_img = tf.keras.utils.array_to_img(image[0])\n",
    "        pytorch_batch = preprocess(pil_img).unsqueeze(0)\n",
    "        tf_batch = np.transpose(pytorch_batch.numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61799382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = 'mobilenet_v2.tflite'\n",
    "with open(tflite_int8_model_path, 'wb') as f:\n",
    "  f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "# Convert PyTorch Input Tensor into Numpy Matrix and Reshape for TensorFlow\n",
    "tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tf_input_data)\n",
    "tfl_interpreter.invoke()\n",
    "\n",
    "tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "probs = softmax(tfl_output_data[0])\n",
    "data = zip(range(len(probs)), probs)\n",
    "tfl_int8_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  tfl_int8_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9ae5b",
   "metadata": {},
   "source": [
    "# Analysing Models\n",
    "\n",
    "Defined below is a function to print out the operator counts of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc21b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the int8 model\n",
    "utils.print_operator_counts(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b76da4",
   "metadata": {},
   "source": [
    "## Compare Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f011e37-c9d2-445d-b4a8-702c0dcb1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# load dataset\n",
    "ds, info = tfds.load('imagenet_v2', split='test', with_info=True, as_supervised=True, shuffle_files=False)\n",
    "ds = ds.shuffle(100, reshuffle_each_iteration=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf134ea-f5aa-4583-a8e2-bfca9071c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_tflite(top_n:int = 1, samples=1000, verbose=False) -> float:\n",
    "    if top_n < 1 or n > 1000:\n",
    "        raise ValueError\n",
    "    \n",
    "    # take subset of dataset\n",
    "    selection = ds.prefetch(10).take(samples)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    for image, label in selection: \n",
    "        trueCatIdx = tf.get_static_value(label)\n",
    "        # convert to PIL.Image\n",
    "        img = tf.keras.utils.array_to_img(image)\n",
    "        \n",
    "        # preprocess using PyTorch functions then convert back into Tf.Tensor\n",
    "        pytorch_batch = preprocess(img).unsqueeze(0)\n",
    "        tf_batch = np.transpose( pytorch_batch.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "        # use same tflite interpreter as before\n",
    "        tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tf_batch)\n",
    "        tfl_interpreter.invoke()\n",
    "\n",
    "        output = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "        # Sort into List[Tuple[index, confidence]] ordered by confidence (descending)\n",
    "        data = sorted(\n",
    "            zip(range(len(output[0])), output[0]),\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "\n",
    "        top_n_results: List[int] = [idx for (idx, _) in data[:top_n]]\n",
    "\n",
    "        if trueCatIdx in top_n_results:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            incorrect = incorrect + 1\n",
    "            if(verbose):\n",
    "                print(\"--incorrect--\")\n",
    "                print(f\"True Category: {categories[trueCatIdx]}({trueCatIdx})\")\n",
    "                print([f\"Top-{top_n} categories: {categories[idx]}({idx})\" for idx in top_n_results])\n",
    "                display(img)\n",
    "    \n",
    "    accuracy = (correct / (correct+incorrect))\n",
    "    print(f\"Top-{top_n} accuracy (TFLite Model): {accuracy * 100}% ({correct}/{correct+incorrect})\")\n",
    "    return accuracy\n",
    "\n",
    "def accuracy_torch(top_n: int = 1, samples=1000, verbose=False):\n",
    "    if top_n < 1 or n > 1000:\n",
    "        raise ValueError\n",
    "    \n",
    "    # take subset of dataset\n",
    "    selection = ds.prefetch(10).take(samples)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    for image, label in selection: \n",
    "        trueCatIdx = tf.get_static_value(label)\n",
    "        \n",
    "        # convert to PIL.Image\n",
    "        img = tf.keras.utils.array_to_img(image)\n",
    "        input_batch = preprocess(img).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = pytorch_model(input_batch)\n",
    "\n",
    "        # Show top categories per image\n",
    "        vals, idxs = torch.topk(output[0], top_n)\n",
    "        \n",
    "        if trueCatIdx in idxs:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            incorrect = incorrect + 1\n",
    "            if(verbose):\n",
    "                print(\"--incorrect--\")\n",
    "                print(f\"True Category: {categories[trueCatIdx]}({trueCatIdx})\")\n",
    "                print([f\"Top-{top_n} categories: {categories[idx]}({idx})\" for idx in top_n_results])\n",
    "                display(img)\n",
    "    \n",
    "    accuracy = (correct / (correct+incorrect))\n",
    "    print(f\"Top-{top_n} accuracy (PyTorch Model): {accuracy * 100}% ({correct}/{correct+incorrect})\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046ed32-c534-46eb-8ee5-87dbd6628873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = 500\n",
    "for n in range(5):\n",
    "    accuracy_torch(n+1, samples)\n",
    "    accuracy_tflite(n+1, samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
