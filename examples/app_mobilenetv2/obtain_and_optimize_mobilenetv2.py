from xmos_ai_tools import xformer
from xmos_ai_tools.xinterpreters import TFLMHostInterpreter
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from save_mobilenet import save_quantized_mobilenet
import numpy as np

HEIGHT, WIDTH, CHANNELS = 160, 160, 3
TFLITE_MODEL_PATH = "mobilenetv2.tflite"
OPT_MODEL_PATH = "src/model.tflite"
OPT_PARAMS_PATH = "src/model_flash.params"
NAMING_PREFIX = "model_"
ALPHA_VALUE = 1.0

###############################################
# Creating and converting a MobileNetV2 model #
###############################################

# Obtain MobileNetV2 model
model = MobileNetV2(
    input_shape=(HEIGHT, WIDTH, CHANNELS),
    alpha=ALPHA_VALUE,
    weights="imagenet",
)

# Quantize the model to int8 and save
save_quantized_mobilenet(model, TFLITE_MODEL_PATH, (HEIGHT, WIDTH))

# Convert the model to XCore optimized TFLite via xformer:
# There are various ways to configure the compiler to optimize the model,
# operator splitting isn't documented yet. This configuration works well for
# MobileNetV2, reach out if you need assistance with other complex models
xformer.convert(
    TFLITE_MODEL_PATH,
    OPT_MODEL_PATH,
    {
        "xcore-flash-image-file": OPT_PARAMS_PATH,
        "xcore-thread-count": "5",
        "xcore-naming-prefix": NAMING_PREFIX,
        "xcore-op-split-tensor-arena": "True",
        "xcore-op-split-top-op": "0",
        "xcore-op-split-bottom-op": "10",
        "xcore-op-split-num-splits": "7",
        "xcore-conv-err-threshold": "3",
    },
)

# Generate flash binary
xformer.generate_flash(
    output_file="xcore_flash_binary.out",
    model_files=[OPT_MODEL_PATH],
    param_files=[OPT_PARAMS_PATH],
)

#######################################################################
# Running the model on xcore host interpreter with sample input image #
#######################################################################

# Sample image of a lion (ImageNet class 291)
with open("lion.bin", "rb") as f:
    data = f.read()

input_array = np.frombuffer(data, dtype=np.uint8)
# input image values are in the range 0 to 255
# we subtract 128 to change to -128 to 127 for int8
input_array = (input_array - 128).astype(np.int8)


interpreter = TFLMHostInterpreter()
interpreter.set_model(model_path=OPT_MODEL_PATH, params_path=OPT_PARAMS_PATH)
interpreter.allocate_tensors()

# Interpreter.get_input_details and interpreter.get_output_details
# return a list for each input/output in the model
# MobileNetV2 only has a single input and output, so we unwrap it
(input_details,) = interpreter.get_input_details()
(output_details,) = interpreter.get_output_details()

input_data = input_array.astype(input_details["dtype"])
input_data = np.reshape(input_data, input_details["shape"])
interpreter.set_tensor(input_details["index"], input_data)

# Inference
interpreter.invoke()
detections = interpreter.get_tensor(output_details["index"])
print(f"Inferred imagenet class = {detections.argmax()}")
