{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53aeff71-1811-4956-95db-13c0ad7f276d",
   "metadata": {},
   "source": [
    "# Training and Converting an LSTM Model from PyTorch to Tensorflow Lite for Micro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a794d61-a6ff-42b3-bc1d-849c2dd2ab75",
   "metadata": {},
   "source": [
    "## Training in PyTorch\n",
    "\n",
    "Let's train a hand-written character recognition model with the MNIST dataset which uses LSTM models.\n",
    "\n",
    "Start off with importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f9774-2b1d-4903-a089-1fd032b6532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\"\"\"\n",
    "Each data point is of form [PIL.Image, class] where the class is a number 0-9 of the\n",
    "respective digit. The image is a single channel image with values from 0 to 1.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mnist_data(train: bool) -> torchvision.datasets.MNIST:\n",
    "    return torchvision.datasets.MNIST(\n",
    "        \"./mnist/\",\n",
    "        download=True,\n",
    "        train=train,\n",
    "        transform=transforms.Compose([transforms.ToTensor()]),\n",
    "    )\n",
    "\n",
    "\n",
    "# iterator returning Tensors of type float32 and shape [batch (10), channels (1), height (28), width (28)]\n",
    "data_loaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        mnist_data(True), batch_size=100, shuffle=True, num_workers=4\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        mnist_data(False), batch_size=100, shuffle=True, num_workers=4\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f54c38-9bd9-4b43-8176-603f1c151256",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2670a68-ae13-4d9e-b9de-297cf74f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):\n",
    "        super(MNIST_LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove batch parameter\n",
    "        x = x.reshape(-1, self.input_size, self.input_size)\n",
    "\n",
    "        # Set initial hidden and cell states\n",
    "        hidden_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        cell_states_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(x, (hidden_initial, cell_states_initial))\n",
    "\n",
    "        dropout_out = self.dropout(lstm_out)\n",
    "\n",
    "        fc_out = self.fc(dropout_out[:, -1, :])\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5304fc-83cd-4bae-8334-5de38e890f56",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4496e-63dd-4753-8a90-004351b15674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "\n",
    "# instantiate model\n",
    "model = MNIST_LSTM(input_size, hidden_size=128)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a154b68-b258-405e-a2eb-a03af2b7361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, model: MNIST_LSTM, loaders):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for i, (images, labels) in enumerate(loaders[\"train\"]):\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"Ended epoch: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adfd31-a7c9-42c8-a604-f39afaa5d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs=3, model=model, loaders=data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6161c20-3be0-4a6b-af5f-e375cc2d7ae8",
   "metadata": {},
   "source": [
    "### Check that the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8748974-a7dd-45f2-a57a-b3414b383733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load one batch to test\n",
    "images, labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "results = model(images).tolist()\n",
    "processed_results = list(map(lambda res: res.index(max(res)), results))\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(images[idx]))\n",
    "    print(f\"{processed_results[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687d372-cd62-4c01-9cd8-30cf67065bd5",
   "metadata": {},
   "source": [
    "### Check Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e71c9-c34d-4d83-9cd2-ff8b75f040df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in data_loaders[\"test\"]:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "\n",
    "        correct = correct + (predicted == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        \"Test Accuracy of the model on the 10000 test images: {} %\".format(\n",
    "            100 * correct / total\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a7040-957d-4f2a-9b0c-f42864cef56a",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb269b-2cf5-4104-baf3-3e1bd24fa114",
   "metadata": {},
   "source": [
    "Now we can convert this to ONNX using PyTorch's built-in exporter. \n",
    "\n",
    "To do this, we need to provide a sample input. This sample input is passed as input to the model and the trace generated will be used to construct the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391abd8-c0d1-4910-87bc-372d760da611",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input, sample_labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "onnx_filename = \"handwriting.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, sample_input, onnx_filename, input_names=[\"images\"], output_names=[\"number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ecc8f-3cab-46e7-b6e2-084b1aa351a0",
   "metadata": {},
   "source": [
    "## Check ONNX\n",
    "Let's just check that the ONNX conversion worked before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cab25e-2e3b-4edd-a6f2-6ada65cb67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc577c33-5f28-4581-84d5-84c0f006299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_filename)\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"images\": sample_input.numpy()},\n",
    ")\n",
    "onnx_processed_results = [numpy.argmax(res) for res in outputs[0]]\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(sample_input[idx]))\n",
    "    print(f\"{onnx_processed_results[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e2af0-ee1e-498e-8293-0bc41834aa4f",
   "metadata": {},
   "source": [
    "## ONNX To TFlite\n",
    "\n",
    "Our ONNX model seems to be working correctly, so let's continue to convert our model into a Keras Model. From there, we can convert it into a TFLite file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7d54e-8598-48fb-ac75-9c55c0903897",
   "metadata": {},
   "source": [
    "### ONNX to Keras\n",
    "\n",
    "Here we use a library by [@PINTO0309](https://github.com/PINTO0309/) to convert our model into a Keras model. For full documentation, see: https://github.com/PINTO0309/onnx2tf\n",
    "\n",
    "`enable_rnn_unroll=True` option unrolls the network.\n",
    "\n",
    "#### Important note about Parameter Replacement\n",
    "\n",
    "`onnx2tf` converts PyTorch Tensors of shape BCHW into the form BHWC, as is convention for TensorFlow. It usually manages this fine, however, if it gets the shapes wrong, you can use the `param_replacement_file` to reshape an operator's input's our outputs to correct it. \n",
    "\n",
    "For example in this example, the `squeeze` after the LSTM operation was changed to operate on the 3rd axis, but the output of the LSTM output was not updated to match; therefore, we add a `post_process_transform` to the LSTM operator to make it work. For more details, see: https://github.com/PINTO0309/onnx2tf#parameter-replacement\n",
    "\n",
    "To see the source of the problem by checking input and output shapes, it may be useful to view the ONNX model in the [Netron](https://netron.app/) visualiser. This is also an easy way to check the `op_name` and `param_name` of the operations you wish to target.\n",
    "\n",
    "For example, if we remove the param_replacement_file, we get the error `Can not squeeze dim[3], expected a dimension of 1, got 128 for '{{node tf.squeeze_3/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[3]](Placeholder)' with input shapes: [100,1,28,128].`. Opening the converted ONNX file, we can see that the output of the LSTM operation has the shape [28, 1, 100, 128] however the error message tells us that it expects the 3rd axis to be 1; this is a good indicator that we need to reshape the output, and sure enough, when we use the param_replacement_file to do this, it works!\n",
    "\n",
    "![screenshot of netron graph](netron-screenshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141257a9-b9de-4ca3-ac18-3aa6cd985652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create param_replacement.json file\n",
    "# (This should be a separate file, but for reader convenience, we create the file in this script)\n",
    "\n",
    "param_replacement_path = \"param_replacement.json\"\n",
    "param_replacement_content = \"\"\"\n",
    "{\n",
    "    \"format_version\": 1,\n",
    "    \"operations\": [\n",
    "        {\n",
    "            \"op_name\": \"LSTM_22\",\n",
    "            \"param_target\": \"outputs\",\n",
    "            \"param_name\": \"onnx::Squeeze_94\",\n",
    "            \"post_process_transpose_perm\": [0, 2, 3, 1]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "with open(param_replacement_path, \"w\") as fh:\n",
    "    fh.write(param_replacement_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d4beb-6978-4dfb-ad1f-a39415f80c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_filename,\n",
    "    output_folder_path=\"handwriting.tf\",\n",
    "    copy_onnx_input_output_names_to_tflite=True,\n",
    "    non_verbose=True,\n",
    "    enable_rnn_unroll=True,\n",
    "    param_replacement_file=param_replacement_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36939950-ceb7-47a9-99e2-0609a2d8c37e",
   "metadata": {},
   "source": [
    "#### Check Keras Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b8e67-e2f2-48d5-83ad-08cfd485db28",
   "metadata": {},
   "source": [
    "Now that we have a Keras model, let's check its accuracy. If all went well, it should not have changed significantly from the original model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f797340-539f-45bc-97ba-862fcea5e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_correct = 0\n",
    "keras_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders[\"test\"]:\n",
    "    # transpose the input_batch into BHWC order for tensorflow\n",
    "    tf_input_data = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    keras_output_data = keras_model(tf_input_data)\n",
    "    processed_keras_output = [numpy.argmax(res) for res in keras_output_data]\n",
    "\n",
    "    keras_results = zip(processed_keras_output, test_labels)\n",
    "    keras_batch_correct = sum(\n",
    "        [1 if output == label.item() else 0 for output, label in keras_results]\n",
    "    )\n",
    "\n",
    "    keras_correct = keras_correct + keras_batch_correct\n",
    "    keras_total = keras_total + test_labels.size(0)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Test Accuracy of the keras model on the {keras_total} test images: {100 * keras_correct / keras_total} % ({keras_correct}/{keras_total})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb30033-ad26-4568-90f3-f85c064f29d3",
   "metadata": {},
   "source": [
    "### Keras to TensorFlow Lite\n",
    "Now let's quantise our model to int8 and export it to a TensorFlow Lite flatbuffer.\n",
    "\n",
    "\n",
    "#### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model for more details on this.](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff7a3f-2826-4a3f-8a77-e9b93a68dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for test_images, _ in data_loaders[\"test\"]:\n",
    "        tf_batch = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043d1fd-b8c2-4184-9305-4a6f22e36549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = \"handwriting.tflite\"\n",
    "with open(tflite_int8_model_path, \"wb\") as f:\n",
    "    f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b447d46-da8b-44e6-88a5-e88fcb538d20",
   "metadata": {},
   "source": [
    "#### Check Accuracy of Converted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bc907-7baa-484d-9314-b49a64b32917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "tfl_correct = 0\n",
    "tfl_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders[\"test\"]:\n",
    "    int_images = (test_images.numpy() * 255 - 128.0).astype(numpy.int8)\n",
    "    # transpose the input_batch into BHWC order for tensorflow\n",
    "    tfl_input_data = numpy.transpose(int_images, [0, 2, 3, 1])\n",
    "\n",
    "    tfl_interpreter.set_tensor(tfl_input_details[0][\"index\"], tfl_input_data)\n",
    "    tfl_interpreter.invoke()\n",
    "\n",
    "    tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0][\"index\"])\n",
    "    tfl_processed_results = [numpy.argmax(res) for res in tfl_output_data]\n",
    "\n",
    "    tfl_results = zip(tfl_processed_results, test_labels)\n",
    "    tfl_batch_correct = sum(\n",
    "        [1 if output == label.item() else 0 for output, label in tfl_results]\n",
    "    )\n",
    "\n",
    "    tfl_correct = tfl_correct + tfl_batch_correct\n",
    "    tfl_total = tfl_total + test_labels.size(0)\n",
    "\n",
    "print(\n",
    "    f\"Test Accuracy of the TensorFlow Lite for Micro model on the {tfl_total} test images: {100 * tfl_correct / tfl_total} % ({tfl_correct}/{tfl_total})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec22f2d-c1dc-41b8-a537-22aa8c052bfb",
   "metadata": {},
   "source": [
    "### Check operator counts of converted model\n",
    "Let us take a look at the operator counts inside the converted model. Ideally, we want the same accuracy with minimal operators. This uses a helper function defined in ../utils, but this step is not necessary to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36764ad8-6643-4227-98e6-fc37e291a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import utils\n",
    "\n",
    "utils.print_operator_counts(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259ccf2-5d14-4b03-8288-a28a8e77c954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
