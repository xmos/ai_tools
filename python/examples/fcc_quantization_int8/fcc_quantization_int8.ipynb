{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(\"Eager execution enabled: \", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and rescale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "scale = tf.constant(255, dtype=tf.dtypes.float32)\n",
    "x_train, x_test = train_images/scale, test_images/scale\n",
    "y_train, y_test = tf.expand_dims(train_labels, 1), tf.expand_dims(test_labels, 1)\n",
    "\n",
    "#mean = tf.math.reduce_mean(x_train)\n",
    "#std = tf.math.reduce_std(x_train)\n",
    "#x_train, x_test = (x_train-mean)/std, (x_test-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define, compile, and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# single dense layer, i.e. multiple logistic regression\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "training_params = {'optimizer': 'adam',\n",
    "                   'loss': 'sparse_categorical_crossentropy',\n",
    "                   'metrics': ['accuracy']}\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "model.compile(**training_params)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TFLite and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = pathlib.Path(\"./mnist_models/\")\n",
    "models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model_float_lite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_float_file = models_dir/\"model_float.tflite\"\n",
    "size_float = model_float_file.write_bytes(model_float_lite)\n",
    "print('Float model size: {:.0f} KB'.format(size_float/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # this doesn't seem to do anything\n",
    "\n",
    "# representative dataset to estimate activation distributions\n",
    "x_train_ds = tf.data.Dataset.from_tensor_slices((x_train)).batch(1)\n",
    "def representative_data_gen():\n",
    "    for input_value in x_train_ds.take(100):\n",
    "        yield [input_value]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "model_quant_lite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant_file = models_dir/\"model_quant.tflite\"\n",
    "size_quant = model_quant_file.write_bytes(model_quant_lite)\n",
    "print('Quantized model size: {:.0f} KB'.format(size_quant/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build interpreters and run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_float = tf.lite.Interpreter(model_content=model_float_lite)\n",
    "interpreter_float.allocate_tensors()\n",
    "interpreter_quant = tf.lite.Interpreter(model_content=model_quant_lite)\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_float = np.NaN*np.zeros((y_test.shape[0], 10))\n",
    "probabilities_quant = np.NaN*np.zeros((y_test.shape[0], 10))\n",
    "probabilities = model(x_test).numpy()\n",
    "\n",
    "for j, img in enumerate(x_test):\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    interpreter_float.set_tensor(interpreter_float.get_input_details()[0][\"index\"], img)\n",
    "    interpreter_float.invoke()\n",
    "    probabilities_float[j] = interpreter_float.get_tensor(interpreter_float.get_output_details()[0][\"index\"])\n",
    "    \n",
    "    interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], img)\n",
    "    interpreter_quant.invoke()\n",
    "    probabilities_quant[j] = interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_abs_err_float = norm(probabilities_float-probabilities, axis=1)\n",
    "prob_abs_err_quant = norm(probabilities_quant-probabilities, axis=1)\n",
    "denom = norm(probabilities, axis=1)\n",
    "prob_rel_err_float = prob_abs_err_float / denom\n",
    "prob_rel_err_quant = prob_abs_err_quant / denom\n",
    "print('Mean relative error of output activations compared to original model output:')\n",
    "print('# Float TFLite model:     {:.5e}'.format(np.mean(prob_rel_err_float)))\n",
    "print('# Quantized TFLite model: {:.5e}'.format(np.mean(prob_rel_err_quant)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_float = np.argmax(probabilities_float, axis=1)\n",
    "predictions_quant = np.argmax(probabilities_quant, axis=1)\n",
    "predictions = np.argmax(probabilities, axis=1)\n",
    "\n",
    "acc = tf.metrics.Accuracy()\n",
    "print('Accuracy of models:')\n",
    "print('# Original keras model:   {:.2%}'.format(acc(test_labels, predictions).numpy()))\n",
    "print('# Float TFLite model:     {:.2%}'.format(acc(test_labels, predictions_float).numpy()))\n",
    "print('# Quantized TFLite model: {:.2%}'.format(acc(test_labels, predictions_quant).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreter surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run interpreters on a single sample\n",
    "img = tf.expand_dims(x_test[10], 0)\n",
    "interpreter_float.set_tensor(interpreter_float.get_input_details()[0][\"index\"], img)\n",
    "interpreter_float.invoke()\n",
    "interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], img)\n",
    "interpreter_quant.invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float interpreter components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_float.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized interpreter components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_quant.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve input image and its quantization, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_float = interpreter_float.get_tensor(1)[0].copy()\n",
    "img_quant_float = interpreter_quant.get_tensor(5)[0].copy()\n",
    "img_quant_int8 = interpreter_quant.get_tensor(1)[0].copy()\n",
    "img_quantization = interpreter_quant.get_tensor_details()[1]['quantization']\n",
    "\n",
    "img_quant_int8_float = (np.float32(img_quant_int8) - img_quantization[1])*img_quantization[0]\n",
    "img_quant_float_int8 = np.int8(img_quant_float/img_quantization[0] + img_quantization[1])\n",
    "img_quant_diff = np.abs((np.float32(img_quant_int8) - img_quantization[1]) * img_quantization[0] - img_quant_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dict = {\"float input\": img_float,\n",
    "           \"quant float input\": img_quant_float,\n",
    "           \"quant int8 input\": img_quant_int8,\n",
    "           \"quant inputs' diff\": img_quant_diff,\n",
    "           \"float from quant int8\": img_quant_int8_float,\n",
    "           \"int8 from quant float\": img_quant_float_int8}\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for j, (title, im) in enumerate(im_dict.items()):\n",
    "    plt.subplot(1, len(im_dict), j+1)\n",
    "    kwargs = {'vmin':0, 'vmax':1} if title == \"quant inputs' diff\" else dict()\n",
    "    plt.imshow(im, cmap='gray', **kwargs)\n",
    "    plt.title(title)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate that the bug corrupts the internal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: file a bug report\n",
    "\n",
    "interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], tf.expand_dims(img_quant_int8_float, 0))\n",
    "interpreter_quant.invoke()\n",
    "print('Output with corrupted image:')\n",
    "print(interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"]).flatten())\n",
    "\n",
    "interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], tf.expand_dims(img_quant_float, 0))\n",
    "interpreter_quant.invoke()\n",
    "print('Output with uncorrupted image:')\n",
    "print(interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"]).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve weights and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_quant = interpreter_quant.get_tensor(3)\n",
    "weights_float = interpreter_float.get_tensor(3)\n",
    "weights_quantization = interpreter_quant.get_tensor_details()[3]['quantization']\n",
    "\n",
    "weights_quant_diff = np.abs(np.float32(weights_quant) - weights_float / weights_quantization[0])\n",
    "weights_rel_err = norm(weights_quant_diff) / norm(np.float32(weights_quant))\n",
    "print('Mean relative error between quantized and float weights: {:.4%}'.format(weights_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights_quant.reshape(-1, 28, 28)\n",
    "plt.figure(figsize=(16, 7))\n",
    "for j in range(10):\n",
    "    plt.subplot(2, 5, j+1)\n",
    "    plt.imshow(w[j,:,:], vmin=-128, vmax=127)\n",
    "    plt.title('Digit {}'.format(j))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of weight quantization errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights_quant_diff.reshape(-1, 28, 28)\n",
    "plt.figure(figsize=(16, 1))\n",
    "for j in range(10):\n",
    "    plt.subplot(1, 10, j+1)\n",
    "    plt.hist(w[j,:,:].reshape(-1))\n",
    "    plt.title('Digit {}'.format(j))\n",
    "plt.subplots_adjust(wspace=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve biases and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_quant = interpreter_quant.get_tensor(4)\n",
    "bias_float = interpreter_float.get_tensor(4)\n",
    "bias_quantization = interpreter_quant.get_tensor_details()[4]['quantization']\n",
    "\n",
    "bias_quant_diff = np.abs(np.float32(bias_quant) - bias_quantization[1] \\\n",
    "                                - bias_float / bias_quantization[0])\n",
    "bias_rel_err = norm(bias_quant_diff) / norm(np.float32(bias_quant))\n",
    "print('Mean relative error between quantized and float matmul bieses: {:.4%}'.format(bias_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve preactivations and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the tensor dense/BiasAdd is actually a (pre)activation, not a bias\n",
    "preact_quant = interpreter_quant.get_tensor(2)\n",
    "preact_float = interpreter_float.get_tensor(2)\n",
    "preact_quantization = interpreter_quant.get_tensor_details()[2]['quantization']\n",
    "\n",
    "preact_quant_diff = np.abs(np.float32(preact_quant) - preact_quantization[1] - preact_float / preact_quantization[0])\n",
    "preact_rel_err = norm(preact_quant_diff) / norm(np.float32(preact_quant))\n",
    "print('Mean relative error between quantized and float preactivations: {:.4%}'.format(preact_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve outputs and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_float = interpreter_float.get_tensor(interpreter_float.get_output_details()[0][\"index\"])\n",
    "output_quant_float = interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"])\n",
    "output_quant_int8 = interpreter_quant.get_tensor(0)\n",
    "output_quantization = interpreter_quant.get_tensor_details()[0]['quantization']\n",
    "\n",
    "output_quant_diff = np.abs(np.float32(output_quant_int8) - output_quantization[1] \\\n",
    "                     - output_float / output_quantization[0])\n",
    "output_rel_err = norm(output_quant_diff) / norm(np.float32(output_quant_int8))\n",
    "print('Mean relative error between quantized and float outputs: {:.4%}'.format(output_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreter reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float interpreter\n",
    "rec_preact_float = np.matmul(weights_float, img_float.flatten()) + bias_float\n",
    "rec_out_float = tf.math.softmax(rec_preact_float).numpy()\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(norm(rec_out_float-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights converted to float and float input (from quant model), compared quant output\n",
    "rec_preact_float2 = np.matmul(\n",
    "    np.float32(weights_quant)*weights_quantization[0],\n",
    "    img_quant_float.flatten()\n",
    ") + bias_quant*bias_quantization[0]\n",
    "\n",
    "rec_out_float2 = tf.math.softmax(rec_preact_float2).numpy()\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float2))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_out_float2-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights converted to float and int input converted to float\n",
    "# NOTE: because of the above bug, float->int8->float converted image is used\n",
    "rec_preact_float3 = np.matmul(\n",
    "    np.float32(weights_quant)*weights_quantization[0],\n",
    "    (np.float32(img_quant_float_int8) - img_quantization[1]).flatten()*img_quantization[0]\n",
    ") + bias_quant*bias_quantization[0]\n",
    "rec_out_float3 = tf.math.softmax(rec_preact_float3).numpy()\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float3))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_out_float3-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights and int input, using 32 bit accumulation and 32 bit bias\n",
    "rec_preact_int32 = np.matmul(np.int32(weights_quant),\n",
    "                           np.int32(img_quant_float_int8).flatten()) \\\n",
    "    - np.matmul(np.int32(weights_quant),\n",
    "                np.int32(img_quantization[1]*np.ones(img_quant_float_int8.size))) \\\n",
    "    + bias_quant\n",
    "rec_out_int = tf.math.softmax(rec_preact_int32*bias_quantization[0]).numpy()\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_int))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_out_int-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XS3 emulation and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XS3VPU import XS3VPU\n",
    "\n",
    "def compute_chunk(vpu, W, x, W_start, W_step, x_start):\n",
    "    # ~ 17 instructions\n",
    "    vpu.VLDC(x[x_start:x_start+vpu.ve]); rw = W_start\n",
    "    for _ in range(vpu.acc_period):  # unroll in asm\n",
    "        vpu.VLMACCR(W[rw:rw+vpu.ve]); rw += W_step\n",
    "\n",
    "def compute_tile(vpu, W, x, N_chunks,\n",
    "                 W_start, W_step, W_chunk_step,\n",
    "                 x_start, x_step):\n",
    "    # ~ N_chunks * (17 + 2) + 5\n",
    "    rx = x_start; rw = W_start\n",
    "    for _ in range(N_chunks):\n",
    "        compute_chunk(vpu, W, x, W_start=rw, W_step=W_step, x_start=rx)\n",
    "        rx += x_step; rw += W_chunk_step\n",
    "\n",
    "def XS3_matmul(vpu, W, x, y, N_bands, N_chunks):\n",
    "    # ~ N_bands * (N_chunks * (17 + 2) + 5 + 8) + 5\n",
    "    rw = 0; ry = 0\n",
    "    for _ in range(N_bands):\n",
    "        vpu.VCLRDR()  # TODO add bias loading\n",
    "        compute_tile(vpu, W, x, N_chunks,\n",
    "                     W_start=rw, W_step=N_chunks*vpu.ve, W_chunk_step=vpu.ve,\n",
    "                     x_start=0, x_step=vpu.ve)\n",
    "        y[ry:ry+vpu.acc_period] = vpu._combine_vD_vR()  # VLSAT, VPOS, VSTRPV\n",
    "        rw += vpu.acc_period * N_chunks * vpu.ve; ry += vpu.acc_period\n",
    "        \n",
    "def XS3_fc_forward_int32(W, b, x):\n",
    "    vpu = XS3VPU(bpe=8)\n",
    "    y = np.zeros((16,), dtype=np.int32).flatten()\n",
    "    XS3_matmul(vpu, W, x, y, N_bands=1, N_chunks=800//vpu.ve)\n",
    "    return y + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad0 = 16-weights_quant.shape[0]\n",
    "pad1 = weights_quant.shape[1] - 32*(weights_quant.shape[1]//32)\n",
    "weights_xs3 = np.pad(weights_quant, pad_width=[(0, pad0), (0, pad1)])\n",
    "weights_xs3 = np.flipud(weights_xs3).flatten()\n",
    "\n",
    "data_xs3 = np.pad(img_quant_float_int8.flatten(), pad_width=[(0, pad1)])\n",
    "\n",
    "bias_xs3 = bias_quant - \\\n",
    "    np.matmul(np.int32(weights_quant),\n",
    "              np.int32(img_quantization[1]*np.ones(img_quant_float_int8.size)))\n",
    "bias_xs3 = np.pad(bias_xs3, pad_width=[(0, pad0)])\n",
    "\n",
    "y_xs3 = XS3_fc_forward_int32(weights_xs3, bias_xs3, data_xs3)\n",
    "\n",
    "preact_xs3_int32 = y_xs3[:-pad0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"int32 preactivation values produced by XS3 emulation (without offset):\")\n",
    "print(preact_xs3_int32)\n",
    "print(\"int32 preactivation values produced by int32 accumulation:\")\n",
    "print(rec_preact_int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to float preactivation\n",
    "rec_preact_xs3 = preact_xs3_int32 * bias_quantization[0]\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed preactivation (xs3):\\n{}\".format(rec_preact_xs3))\n",
    "    print(\"Original preactivation:\\n{}\".format(preact_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_preact_xs3-preact_float.flatten())/norm(preact_float.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating final bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sat_to_16(a):\n",
    "    return np.int16(np.round(np.clip(a, -2**15+1, 2**15-1)))\n",
    "\n",
    "def sat_to_8(a):\n",
    "    return np.int8(np.round(np.clip(a, -2**7+1, 2**7-1)))\n",
    "\n",
    "def XS3_fc_forward_int8(W, b, x, rshift, scale):\n",
    "    y = XS3_fc_forward_int32(W, b, x)\n",
    "    preact_xs3_int32_offset = y[:-pad0]\n",
    "    preact_xs3_int32_vlsat = sat_to_16(preact_xs3_int32_offset / 2**rshift)\n",
    "    preact_xs3_int32_vlmul = sat_to_16(preact_xs3_int32_vlsat * scale  / 2**14)\n",
    "    return sat_to_8(preact_xs3_int32_vlmul / 2**7)  # this is what VDEPTH8 would do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final bias is calculated here\n",
    "# this includes the output offset, so that fused activations are already applied\n",
    "bias_scale = bias_quantization[0]\n",
    "output_scale, output_zero_point = preact_quantization\n",
    "multiplier = bias_scale / output_scale\n",
    "\n",
    "rshift = -np.ceil(np.log2(multiplier))\n",
    "scale = np.round(2**14 * (multiplier * 2**rshift))\n",
    "if scale == 2**14:\n",
    "    rshift -= 1\n",
    "    scale /= 2\n",
    "rshift -= 7\n",
    "    \n",
    "bias_xs3_offset = bias_xs3 + np.int32(output_zero_point / multiplier)\n",
    "\n",
    "preact_xs3_int8 = XS3_fc_forward_int8(weights_xs3, bias_xs3_offset, data_xs3, rshift, scale)\n",
    "\n",
    "print(\"int8 preactivation values using xs3 emulation:\")\n",
    "print(preact_xs3_int8)\n",
    "print(\"int8 preactivation values produced by tflite:\")\n",
    "print(preact_quant[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of the XS3 emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def eval_pred(args):\n",
    "    j, im = args\n",
    "    data_xs3 = np.pad(np.int8(im+img_quantization[1]).flatten(), pad_width=[(0, pad1)])\n",
    "    preact_xs3_int8 = XS3_fc_forward_int8(weights_xs3, bias_xs3_offset, data_xs3, rshift, scale)\n",
    "    if (j+1) % 10 == 0:\n",
    "        print('{:6d}/10000'.format(j+1))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return np.argmax(preact_xs3_int8)\n",
    "\n",
    "predictions_xs3 = np.zeros(predictions.shape, dtype=np.int64)\n",
    "p = Pool(10)\n",
    "predictions_xs3 = p.map(eval_pred, enumerate(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = tf.metrics.Accuracy()\n",
    "print('Accuracy of models:')\n",
    "print('# Original keras model:   {:.2%}'.format(acc(test_labels, predictions).numpy()))\n",
    "print('# Float TFLite model:     {:.2%}'.format(acc(test_labels, predictions_float).numpy()))\n",
    "print('# Quantized TFLite model: {:.2%}'.format(acc(test_labels, predictions_quant).numpy()))\n",
    "print('# Emulated XS3 model:     {:.2%}'.format(acc(test_labels, predictions_xs3).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
