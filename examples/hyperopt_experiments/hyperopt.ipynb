{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopt notebook\n",
    "Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.\n",
    "In this notebook, the basics are covered and advanced features will be explored. Everything here will be done with the idea of applying hyperopt to hyper-parameter tuning of Keras models (usin TF2.1) (although generalization is persued for other cases as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosmetic imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "# real imports\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import OrderedDict\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval, plotting, STATUS_OK, STATUS_FAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.- Objective functions\n",
    "Test functions for optimization:\n",
    "- [Himmelblau function][https://en.wikipedia.org/wiki/Himmelblau%27s_function]: Himmelblau, D.(1972). Applied Nonlinear Programming.\n",
    "- [Eggholder function][https://www.researchgate.net/publication/337947149_Hybridization_of_interval_methods_and_evolutionary_algorithms_for_solving_difficult_optimization_problems]: Vanaret, Charlie. (2015). Hybridization of interval methods and evolutionary algorithms for solving difficult optimization problems. \n",
    "\n",
    "In applied mathematics, test functions, known as artificial landscapes, are useful to evaluate characteristics of optimization algorithms. These functions will be used to test hyperopt basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau_function(x, y):\n",
    "    return (x**2 + y -11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "def eggholder_function(x, y):\n",
    "    return -(y + 47)*np.sin(np.sqrt(np.abs((x/2) + (y + 47)))) - x*np.sin(np.sqrt(np.abs(x - y + 47)))\n",
    "\n",
    "# Plotting function\n",
    "def plot_3d_surface(f,  limits, angle=90):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    # Make data\n",
    "    lower_, upper_, step_ = limits\n",
    "    X = np.arange(lower_, upper_, step_)\n",
    "    Y = np.arange(lower_, upper_, step_)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    Z = f(X, Y)\n",
    "    # Plot the surface.\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False,\n",
    "                          rstride=1, cstride=1)\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(np.min(Z), np.max(Z))\n",
    "    ax.zaxis.set_major_locator(LinearLocator(4))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax.view_init(30, angle)\n",
    "    plt.show()\n",
    "plot_3d_surface(himmelblau_function, [-5, 5, 0.25], 80)\n",
    "himmelblau_function(3,2)\n",
    "#plot_3d_surface(eggholder_function, [-512, 512, 1], 45)\n",
    "#eggholder_function(512, 404.2319)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.- Hyperopt example\n",
    "### _From their github page_\n",
    "- Objective function\n",
    "- Search space\n",
    "- fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an objective function\n",
    "def objective(args):\n",
    "    case, val = args\n",
    "    if case == 'case 1':\n",
    "        return val\n",
    "    else:\n",
    "        return val ** 2\n",
    "\n",
    "# define a search space\n",
    "space = hp.choice(\n",
    "    'a',\n",
    "    [\n",
    "        ('case 1', 1 + hp.lognormal('c1', 0, 1)),\n",
    "        ('case 2', hp.uniform('name', -10, 10))\n",
    "    ])\n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=2000)\n",
    "print(best)\n",
    "# -> {'a': 1, 'c2': 0.01420615366247227}\n",
    "print(space_eval(space, best))\n",
    "# -> ('case 2', 0.01420615366247227}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.- Search space\n",
    "A search space is a dictionary or OrderedDict as in this case with the hyper-params ranges.\n",
    "### 2.1.- Uniform range or prob distribution (float)\n",
    "    - hp.randint(label, upper)\n",
    "    - hp.uniform(label, low, high)\n",
    "    - hp.loguniform(label, low, high)\n",
    "    - hp.normal(label, mu, sigma)\n",
    "    - hp.lognormal(label, mu, sigma)\n",
    "### 2.2.- Quantized parameters (int)\n",
    "    - hp.quniform(label, low, high, q)\n",
    "    - hp.qloguniform(label, low, high, q)\n",
    "    - hp.qnormal(label, mu, sigma, q)\n",
    "    - hp.qlognormal(label, mu, sigma, q)\n",
    "### 2.3.- Categorical parameters (choices)\n",
    "    - hp.choice(label, [\"list\", \"of\", \"potential\", \"choices\"])\n",
    "    - hp.choice(label, [hp.uniform(sub_label_1, low, high), hp.normal(sub_label_2, mu, sigma), None, 0, 1, \"anything\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_SPACE = OrderedDict([('learning_rate',\n",
    "                             hp.loguniform('learning_rate', np.log(0.01), np.log(0.5))),\n",
    "                            ('epochs',\n",
    "                             hp.choice('epochs', range(1, 51, 1))),\n",
    "                            ('batch_size',\n",
    "                             hp.choice('batch_size', [32, 64, 128, 256, 512])),\n",
    "                            ('l1_reg',\n",
    "                             hp.choice('l1_reg', np.arange(1e-5, 2e-4, 1e-6)))\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Objective function\n",
    "Hyperopt minimizes the function, so change the sign if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    all_params = {**params}\n",
    "    return -1.0 * train_evaluate(all_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.- Data and model declaration:\n",
    "\n",
    "- Downloading the MNIST data for our model\n",
    "- Create a function for building the model with the selected hyper parameters (logistic regression model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite2xcore.model_generation import utils\n",
    "import tensorflow as tf\n",
    "utils.set_all_seeds(42)\n",
    "# Data\n",
    "data = utils.prepare_MNIST(False, simard=False, padding=0)\n",
    "for k, v in data.items():\n",
    "    print(f\"Prepped data[{k}] with shape: {v.shape}\")\n",
    "\n",
    "# Model, called from the objective function\n",
    "def train_evaluate(params):\n",
    "    core_model = tf.keras.Sequential(\n",
    "        name='logistic_regression',\n",
    "        layers=[\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28, 1), name='input'),\n",
    "            tf.keras.layers.Dense(10,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(params['l1_reg']))\n",
    "        ]\n",
    "    )\n",
    "    core_model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=params['learning_rate']),\n",
    "        metrics=['accuracy'])\n",
    "    core_model.fit(\n",
    "        data['x_train'], data['y_train'],\n",
    "        validation_data=(data['x_test'], data['y_test']),\n",
    "        batch_size=params['batch_size'],\n",
    "        verbose=0,\n",
    "        epochs=params['epochs']\n",
    "    )\n",
    "    _, accuracy = core_model.evaluate(data['x_test'], data['y_test'])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.- Run trials\n",
    "- Declare constants MAX_EVAL and instantiate the Trial object\n",
    "- Run fmin to find the best candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "MAX_EVALS = 1000\n",
    "HPO_PARAMS = {'max_evals': MAX_EVALS,\n",
    "              'trials': trials\n",
    "             }\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=SEARCH_SPACE,\n",
    "    algo=tpe.rand.suggest,\n",
    "    **HPO_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.- Minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found minimum after {HPO_PARAMS['max_evals']} trials:\")\n",
    "print(space_eval(SEARCH_SPACE, best))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.2.- Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = space_eval(SEARCH_SPACE, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.- Accuracy range and median (50 trainings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "l = [-objective(best_params) for e in range(50)]\n",
    "maxv = np.max(l)\n",
    "minv = np.min(l)\n",
    "print(f\"Acc range: ({maxv}, {minv})\\nAcc median: {np.median(l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Save/load trials pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(trials, 'hyperopt_trials.pkl') # imagine max_evals = 100\n",
    "trials = joblib.load('./hyperopt_trials.pkl')\n",
    "#_ = fmin(objective, SPACE, trials=trials, algo=tpe.rand.suggest, max_evals=200) -> we can add more trials"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Visualization:\n",
    "- History\n",
    "- Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.main_plot_history(trials)\n",
    "plotting.main_plot_histogram(trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}